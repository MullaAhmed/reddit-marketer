{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Marketing AI Agent - Self-Sufficient Example Cells\n",
    "\n",
    "Each cell in this notebook is **self-sufficient** and can be run independently without running other cells first.\n",
    "\n",
    "## Features:\n",
    "- **Independent Cells**: Each cell initializes its own services and data\n",
    "- **No Dependencies**: Cells don't rely on variables from previous cells\n",
    "- **Direct Functions**: Uses base functions instead of API calls\n",
    "- **No Error Handling**: Clean, simple code without try/catch blocks\n",
    "\n",
    "## Usage:\n",
    "- Run any cell individually\n",
    "- Modify constants in each cell as needed\n",
    "- Each cell demonstrates a specific feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Reddit Marketing AI Agent - Example Workflow (Direct Functions)\n",
      "üìÖ Started at: 2025-06-21 07:43:58.493015\n",
      "üè¢ Organization: Example Organization (example-org-2024)\n",
      "‚ö†Ô∏è  Reddit Posting: ENABLED\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment Check\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.core.settings import settings\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üîç Environment Check\")\n",
    "print(f\"üìÖ Current Time: {datetime.now()}\")\n",
    "print(f\"üìÅ Data Directory: {settings.DATA_DIR}\")\n",
    "print(f\"ü§ñ Default Model: {settings.MODEL_NAME}\")\n",
    "print(f\"üîç Embedding Provider: {settings.EMBEDDING_PROVIDER}\")\n",
    "\n",
    "required_keys = {\n",
    "    \"OPENAI_API_KEY\": settings.OPENAI_API_KEY,\n",
    "    \"GOOGLE_API_KEY\": settings.GOOGLE_API_KEY\n",
    "}\n",
    "\n",
    "print(\"\\nRequired API Keys:\")\n",
    "for key, value in required_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Missing'}\")\n",
    "\n",
    "optional_keys = {\n",
    "    \"GROQ_API_KEY\": settings.GROQ_API_KEY,\n",
    "    \"FIRECRAWL_API_KEY\": settings.FIRECRAWL_API_KEY,\n",
    "    \"LANGCHAIN_PROJECT\": settings.LANGCHAIN_PROJECT\n",
    "}\n",
    "\n",
    "print(\"\\nOptional API Keys:\")\n",
    "for key, value in optional_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Not set'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¢ Setting up Organization\n",
      "‚úÖ Organization: Example Organization\n",
      "üìä ID: example-org-2024\n",
      "üìÑ Documents: 0\n",
      "üìÖ Created: 2025-06-21 02:14:13.899903+00:00\n",
      "üîÑ Active: True\n",
      "\n",
      "üìã Total organizations in system: 1\n",
      "   - Example Organization (example-org-2024): 0 documents\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Create Organization\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.document_service import DocumentService\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "ORGANIZATION_NAME = \"Demo Organization\"\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "\n",
    "# Create organization\n",
    "organization = document_service.get_or_create_organization(ORGANIZATION_ID, ORGANIZATION_NAME)\n",
    "\n",
    "print(f\"üè¢ Organization Created/Retrieved\")\n",
    "print(f\"   Name: {organization.name}\")\n",
    "print(f\"   ID: {organization.id}\")\n",
    "print(f\"   Documents: {organization.documents_count}\")\n",
    "print(f\"   Created: {organization.created_at}\")\n",
    "print(f\"   Active: {organization.is_active}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Ingest Direct Content\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.document_service import DocumentService\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "\n",
    "# Document content\n",
    "documents = [{\n",
    "    \"title\": \"Python Best Practices\",\n",
    "    \"content\": \"\"\"\n",
    "    Python Best Practices for Clean Code\n",
    "    \n",
    "    1. Follow PEP 8 Style Guide\n",
    "    - Use 4 spaces for indentation\n",
    "    - Keep lines under 79 characters\n",
    "    - Use descriptive variable names\n",
    "    \n",
    "    2. Write Docstrings\n",
    "    - Document all functions and classes\n",
    "    - Use triple quotes for docstrings\n",
    "    \n",
    "    3. Use Type Hints\n",
    "    - Add type hints to function parameters\n",
    "    - Use typing module for complex types\n",
    "    \n",
    "    4. Error Handling\n",
    "    - Use specific exception types\n",
    "    - Handle exceptions gracefully\n",
    "    \n",
    "    5. Testing\n",
    "    - Write unit tests for all functions\n",
    "    - Use pytest for testing framework\n",
    "    \"\"\",\n",
    "    \"metadata\": {\"category\": \"programming\", \"language\": \"python\"}\n",
    "}]\n",
    "\n",
    "# Ingest documents\n",
    "success, message, document_ids = document_service.ingest_documents(\n",
    "    documents=documents,\n",
    "    org_id=ORGANIZATION_ID\n",
    ")\n",
    "\n",
    "print(f\"üìÑ Direct Content Ingestion\")\n",
    "print(f\"   Success: {success}\")\n",
    "print(f\"   Message: {message}\")\n",
    "print(f\"   Document IDs: {document_ids}\")\n",
    "print(f\"   Documents Ingested: {len(document_ids) if document_ids else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Method 3: URL Scraping\n",
      "\n",
      "üîç Scraping: https://docs.python.org/3/tutorial/introduction.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:01,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã URL Scraping: Python Tutorial Introduction\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested document from URL: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä document_id: 8f51e877-0741-46ff-a985-70074c1f8ba3\n",
      "üìä url: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä scraping_method: auto\n",
      "\n",
      "üìù Stored 1 document IDs from URL scraping\n",
      "\n",
      "üìö Total documents ingested: 4\n",
      "Document IDs: ['df433591-c4ff-499b-84ea-4d9c087a9fc2', 'e2b78f2d-ec18-40a6-9ffd-9eaf861d1734', '710bc20a-ddfa-4922-a9c2-c166f7e04f53', '8f51e877-0741-46ff-a985-70074c1f8ba3']\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Ingest from URL\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.document_service import DocumentService\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "URL_TO_SCRAPE = \"https://docs.python.org/3/tutorial/introduction.html\"\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "\n",
    "# Ingest from URL\n",
    "async def ingest_url():\n",
    "    success, message, document_id = await document_service.ingest_document_from_url(\n",
    "        url=URL_TO_SCRAPE,\n",
    "        organization_id=ORGANIZATION_ID,\n",
    "        title=\"Python Tutorial Introduction\",\n",
    "        scraping_method=\"auto\"\n",
    "    )\n",
    "    \n",
    "    print(f\"üåê URL Ingestion\")\n",
    "    print(f\"   URL: {URL_TO_SCRAPE}\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    print(f\"   Document ID: {document_id}\")\n",
    "\n",
    "# Run the async function\n",
    "await ingest_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Organization Summary:\n",
      "   Name: Example Organization\n",
      "   Documents: 4\n",
      "   Created: 2025-06-21 02:14:13.899903+00:00\n",
      "\n",
      "üìà Organization Statistics:\n",
      "   Total Documents: 4\n",
      "   Total Chunks: 17\n",
      "   Total Content Length: 25,377 characters\n",
      "   Average Chunks per Document: 4.2\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Query Documents\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.document_service import DocumentService\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "from app.models.document import DocumentQuery\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "SEARCH_QUERY = \"python best practices\"\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "\n",
    "# Create query\n",
    "query = DocumentQuery(\n",
    "    query=SEARCH_QUERY,\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    method=\"semantic\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "# Execute query\n",
    "response = document_service.query_documents(query)\n",
    "\n",
    "print(f\"üîç Document Query\")\n",
    "print(f\"   Query: {response.query}\")\n",
    "print(f\"   Method: {response.method}\")\n",
    "print(f\"   Results: {response.total_results}\")\n",
    "print(f\"   Processing Time: {response.processing_time_ms:.2f}ms\")\n",
    "\n",
    "print(f\"\\nüìÑ Found Documents:\")\n",
    "for i, doc in enumerate(response.documents, 1):\n",
    "    print(f\"   {i}. {doc.title} (Score: {doc.score:.3f})\")\n",
    "    print(f\"      Content: {doc.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 1: Discovering Topics from Documents\n",
      "\n",
      "üìã Topic Discovery\n",
      "----------------------------------------\n",
      "‚úÖ Status: Extracted 10 topics from 3 documents\n",
      "üìä topics: ['Python best practices', 'PEP 8 style guide', 'Machine learning', 'Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Web development with Python', 'FastAPI', 'Python testing', 'Error handling in Python']\n",
      "üìä selected_document_ids: ['df433591-c4ff-499b-84ea-4d9c087a9fc2', 'e2b78f2d-ec18-40a6-9ffd-9eaf861d1734', '710bc20a-ddfa-4922-a9c2-c166f7e04f53']\n",
      "üìä total_topics: 10\n",
      "\n",
      "üìã Discovered Topics:\n",
      "   1. Python best practices\n",
      "   2. PEP 8 style guide\n",
      "   3. Machine learning\n",
      "   4. Supervised learning\n",
      "   5. Unsupervised learning\n",
      "   6. Reinforcement learning\n",
      "   7. Web development with Python\n",
      "   8. FastAPI\n",
      "   9. Python testing\n",
      "   10. Error handling in Python\n",
      "\n",
      "üìä Campaign Status: CampaignStatus.DOCUMENTS_UPLOADED\n",
      "üìÑ Documents Selected: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create Campaign\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "from app.models.campaign import CampaignCreateRequest, ResponseTone\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\"\n",
    "}\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"]\n",
    ")\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "\n",
    "# Create campaign\n",
    "async def create_campaign():\n",
    "    request = CampaignCreateRequest(\n",
    "        name=\"Python Community Outreach\",\n",
    "        description=\"Engage with Python learning communities\",\n",
    "        response_tone=ResponseTone.HELPFUL,\n",
    "        max_responses_per_day=5\n",
    "    )\n",
    "    \n",
    "    success, message, campaign = await campaign_service.create_campaign(\n",
    "        organization_id=ORGANIZATION_ID,\n",
    "        request=request\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Campaign Creation\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    if campaign:\n",
    "        print(f\"   Campaign ID: {campaign.id}\")\n",
    "        print(f\"   Name: {campaign.name}\")\n",
    "        print(f\"   Status: {campaign.status}\")\n",
    "        print(f\"   Tone: {campaign.response_tone}\")\n",
    "    \n",
    "    await campaign_service.cleanup()\n",
    "\n",
    "await create_campaign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Step 2: Discovering Subreddits from Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/positivereinforcement: received 403 HTTP response\n",
      "Error getting details for r/positivereinforcement: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Subreddit Discovery\n",
      "----------------------------------------\n",
      "‚úÖ Status: Discovered 10 relevant subreddits\n",
      "üìä subreddits: ['Python', 'learnpython', 'pythontips', 'MachineLearning', 'learnmachinelearning', 'datascience', 'artificial', 'ArtificialInteligence', 'deeplearning', 'FastAPI']\n",
      "üìä topics: ['Python best practices', 'PEP 8 style guide', 'Machine learning', 'Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Web development with Python', 'FastAPI', 'Python testing', 'Error handling in Python']\n",
      "üìä total_found: 10\n",
      "\n",
      "üéØ Target Subreddits:\n",
      "   1. r/Python\n",
      "   2. r/learnpython\n",
      "   3. r/pythontips\n",
      "   4. r/MachineLearning\n",
      "   5. r/learnmachinelearning\n",
      "   6. r/datascience\n",
      "   7. r/artificial\n",
      "   8. r/ArtificialInteligence\n",
      "   9. r/deeplearning\n",
      "   10. r/FastAPI\n",
      "\n",
      "üìä Campaign Status: CampaignStatus.SUBREDDITS_DISCOVERED\n",
      "üéØ Subreddits Found: 10\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Discover Topics\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "from app.models.campaign import SubredditDiscoveryRequest\n",
    "\n",
    "# Configuration - Replace with actual IDs from previous cells\n",
    "CAMPAIGN_ID = \"your_campaign_id_here\"  # Replace with actual campaign ID\n",
    "DOCUMENT_IDS = [\"your_document_id_here\"]  # Replace with actual document IDs\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\"\n",
    "}\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"]\n",
    ")\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "\n",
    "# Discover topics\n",
    "async def discover_topics():\n",
    "    request = SubredditDiscoveryRequest(document_ids=DOCUMENT_IDS)\n",
    "    \n",
    "    success, message, data = await campaign_service.discover_topics(\n",
    "        campaign_id=CAMPAIGN_ID,\n",
    "        request=request\n",
    "    )\n",
    "    \n",
    "    print(f\"üîç Topic Discovery\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    if data and \"topics\" in data:\n",
    "        print(f\"   Topics Found: {len(data['topics'])}\")\n",
    "        for i, topic in enumerate(data[\"topics\"], 1):\n",
    "            print(f\"      {i}. {topic}\")\n",
    "    \n",
    "    await campaign_service.cleanup()\n",
    "\n",
    "await discover_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Discover Subreddits\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "from app.models.campaign import SubredditDiscoveryByTopicsRequest\n",
    "\n",
    "# Configuration\n",
    "CAMPAIGN_ID = \"your_campaign_id_here\"  # Replace with actual campaign ID\n",
    "TOPICS = [\"python\", \"programming\", \"coding\", \"software development\"]  # Example topics\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\"\n",
    "}\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"]\n",
    ")\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "\n",
    "# Discover subreddits\n",
    "async def discover_subreddits():\n",
    "    request = SubredditDiscoveryByTopicsRequest(topics=TOPICS)\n",
    "    \n",
    "    success, message, data = await campaign_service.discover_subreddits(\n",
    "        campaign_id=CAMPAIGN_ID,\n",
    "        request=request\n",
    "    )\n",
    "    \n",
    "    print(f\"üéØ Subreddit Discovery\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    if data and \"subreddits\" in data:\n",
    "        print(f\"   Subreddits Found: {len(data['subreddits'])}\")\n",
    "        for i, subreddit in enumerate(data[\"subreddits\"], 1):\n",
    "            print(f\"      {i}. r/{subreddit}\")\n",
    "    \n",
    "    await campaign_service.cleanup()\n",
    "\n",
    "await discover_subreddits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Search Subreddits\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.clients.reddit_client import RedditClient\n",
    "\n",
    "# Configuration\n",
    "SEARCH_QUERY = \"python programming\"\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\"\n",
    "}\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"]\n",
    ")\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "\n",
    "# Search subreddits\n",
    "async def search_subreddits():\n",
    "    success, message, results = await reddit_service.search_subreddits(SEARCH_QUERY, limit=5)\n",
    "    \n",
    "    print(f\"üîç Subreddit Search\")\n",
    "    print(f\"   Query: {SEARCH_QUERY}\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    print(f\"   Results: {len(results) if results else 0}\")\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nüéØ Found Subreddits:\")\n",
    "        for i, subreddit in enumerate(results, 1):\n",
    "            print(f\"   {i}. r/{subreddit['name']} ({subreddit['subscribers']:,} subscribers)\")\n",
    "            print(f\"      Description: {subreddit['description'][:80]}...\")\n",
    "    \n",
    "    await reddit_service.cleanup()\n",
    "\n",
    "await search_subreddits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Generate LLM Response\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.llm_service import LLMService\n",
    "from app.clients.llm_client import LLMClient\n",
    "\n",
    "# Configuration\n",
    "PROMPT = \"Explain the benefits of using Python for web development\"\n",
    "\n",
    "# Initialize services\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "\n",
    "# Generate response\n",
    "async def generate_response():\n",
    "    response = await llm_service.generate_completion(\n",
    "        prompt=PROMPT,\n",
    "        response_format=\"text\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"ü§ñ LLM Response Generation\")\n",
    "    print(f\"   Prompt: {PROMPT}\")\n",
    "    print(f\"   Response Length: {len(str(response))} characters\")\n",
    "    print(f\"\\nüìù Generated Response:\")\n",
    "    print(f\"   {str(response)[:200]}...\")\n",
    "\n",
    "await generate_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Extract Topics from Content\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.llm_service import LLMService\n",
    "from app.clients.llm_client import LLMClient\n",
    "\n",
    "# Configuration\n",
    "CONTENT = \"\"\"\n",
    "Python is a versatile programming language that's great for web development, \n",
    "data science, machine learning, and automation. It has frameworks like Django \n",
    "and Flask for web development, pandas and numpy for data analysis, and \n",
    "scikit-learn for machine learning. Python is also popular for DevOps and \n",
    "system administration tasks.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize services\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "\n",
    "# Extract topics\n",
    "async def extract_topics():\n",
    "    success, message, topics = await llm_service.extract_topics_from_content(CONTENT)\n",
    "    \n",
    "    print(f\"üîç Topic Extraction\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    print(f\"   Topics Found: {len(topics) if topics else 0}\")\n",
    "    \n",
    "    if topics:\n",
    "        print(f\"\\nüìã Extracted Topics:\")\n",
    "        for i, topic in enumerate(topics, 1):\n",
    "            print(f\"   {i}. {topic}\")\n",
    "\n",
    "await extract_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Get Analytics\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.analytics_service import AnalyticsService\n",
    "from app.managers.analytics_manager import AnalyticsManager\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "analytics_manager = AnalyticsManager(campaign_manager, document_manager)\n",
    "analytics_service = AnalyticsService(analytics_manager)\n",
    "\n",
    "# Get analytics\n",
    "quick_stats = analytics_service.get_quick_stats(ORGANIZATION_ID)\n",
    "platform_overview = analytics_service.get_overall_platform_metrics()\n",
    "\n",
    "print(f\"üìä Analytics Dashboard\")\n",
    "print(f\"   Organization: {ORGANIZATION_ID}\")\n",
    "\n",
    "if \"error\" not in quick_stats:\n",
    "    print(f\"\\nüìà Quick Stats:\")\n",
    "    print(f\"   Total Campaigns: {quick_stats.get('total_campaigns', 0)}\")\n",
    "    print(f\"   Active Campaigns: {quick_stats.get('active_campaigns', 0)}\")\n",
    "    print(f\"   Total Documents: {quick_stats.get('total_documents', 0)}\")\n",
    "    print(f\"   Success Rate: {quick_stats.get('success_rate', 0):.1f}%\")\n",
    "\n",
    "if \"error\" not in platform_overview:\n",
    "    campaign_stats = platform_overview.get(\"campaign_stats\", {})\n",
    "    print(f\"\\nüåê Platform Overview:\")\n",
    "    print(f\"   Total Campaigns: {campaign_stats.get('total_campaigns', 0)}\")\n",
    "    print(f\"   Total Organizations: {campaign_stats.get('total_organizations', 0)}\")\n",
    "    print(f\"   Active Campaigns: {campaign_stats.get('active_campaigns', 0)}\")\n",
    "    print(f\"   Platform Insights: {len(platform_overview.get('platform_insights', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: List Organizations\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.document_service import DocumentService\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "\n",
    "# List organizations\n",
    "organizations = document_service.list_organizations()\n",
    "\n",
    "print(f\"üè¢ Organizations List\")\n",
    "print(f\"   Total Organizations: {len(organizations)}\")\n",
    "\n",
    "if organizations:\n",
    "    print(f\"\\nüìã Organizations:\")\n",
    "    for i, org in enumerate(organizations, 1):\n",
    "        print(f\"   {i}. {org.name} ({org.id})\")\n",
    "        print(f\"      Documents: {org.documents_count}\")\n",
    "        print(f\"      Created: {org.created_at}\")\n",
    "        print(f\"      Active: {org.is_active}\")\nelse:\n",
    "    print(\"   No organizations found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: List Campaigns\n",
    "import sys\n",
    "import asyncio\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"demo-org-2024\"\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\"\n",
    "}\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "web_scraper_service = WebScraperService()\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "llm_client = LLMClient()\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"]\n",
    ")\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "\n",
    "# List campaigns\n",
    "async def list_campaigns():\n",
    "    success, message, campaigns = await campaign_service.list_campaigns(ORGANIZATION_ID)\n",
    "    \n",
    "    print(f\"üìã Campaigns List\")\n",
    "    print(f\"   Organization: {ORGANIZATION_ID}\")\n",
    "    print(f\"   Success: {success}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    print(f\"   Total Campaigns: {len(campaigns) if campaigns else 0}\")\n",
    "    \n",
    "    if campaigns:\n",
    "        print(f\"\\nüéØ Campaigns:\")\n",
    "        for i, campaign in enumerate(campaigns, 1):\n",
    "            print(f\"   {i}. {campaign.name} ({campaign.status})\")\n",
    "            print(f\"      ID: {campaign.id}\")\n",
    "            print(f\"      Created: {campaign.created_at}\")\n",
    "            print(f\"      Tone: {campaign.response_tone}\")\n",
    "    \n",
    "    await campaign_service.cleanup()\n",
    "\n",
    "await list_campaigns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Web Scraping Test\n",
    "import sys\n",
    "sys.path.append('app')\n",
    "\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "# Configuration\n",
    "TEST_URL = \"https://httpbin.org/html\"  # Simple test URL\n",
    "SCRAPING_METHOD = \"requests\"  # Use requests method for reliability\n",
    "\n",
    "# Initialize service\n",
    "web_scraper = WebScraperService()\n",
    "\n",
    "# Test scraping\n",
    "scraped_content = web_scraper.scrape_url(TEST_URL, method=SCRAPING_METHOD)\n",
    "\n",
    "print(f\"üåê Web Scraping Test\")\n",
    "print(f\"   URL: {TEST_URL}\")\n",
    "print(f\"   Method: {SCRAPING_METHOD}\")\n",
    "print(f\"   Success: {scraped_content is not None}\")\n",
    "\n",
    "if scraped_content:\n",
    "    print(f\"   Content Length: {len(scraped_content)} characters\")\n",
    "    print(f\"\\nüìÑ Content Preview:\")\n",
    "    print(f\"   {scraped_content[:200]}...\")\nelse:\n",
    "    print(f\"   Failed to scrape content\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
