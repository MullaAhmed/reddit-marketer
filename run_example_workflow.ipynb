{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Marketing AI Agent - Complete Example Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow of the Reddit Marketing AI Agent, from document ingestion to response execution.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Setup & Configuration** - Environment validation and service initialization\n",
    "2. **Organization Setup** - Create and configure an organization\n",
    "3. **Document Ingestion** - Multiple methods (direct content, file upload, URL scraping)\n",
    "4. **Campaign Creation** - Create and configure a marketing campaign\n",
    "5. **Topic Discovery** - Extract relevant topics from documents\n",
    "6. **Subreddit Discovery** - Find relevant subreddits based on topics\n",
    "7. **Post Discovery** - Find relevant posts in target subreddits\n",
    "8. **Response Generation** - AI-generated contextual responses\n",
    "9. **Response Execution** - Post responses to Reddit (with safety controls)\n",
    "10. **Analytics & Reporting** - Comprehensive performance analysis\n",
    "\n",
    "## Safety Features:\n",
    "- **Reddit Posting Control**: `ACTUALLY_POST_TO_REDDIT = False` prevents accidental posting\n",
    "- **Credential Validation**: Checks for required API keys\n",
    "- **Error Handling**: Graceful handling of API failures\n",
    "- **Independent Cells**: Each step can be run independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = \"http://localhost:8000/api/v1\"\n",
    "ORGANIZATION_ID = \"example-org-2024\"\n",
    "ORGANIZATION_NAME = \"Example Organization\"\n",
    "\n",
    "# Safety control - Set to True only when you want to actually post to Reddit\n",
    "ACTUALLY_POST_TO_REDDIT = False\n",
    "\n",
    "# Reddit credentials (replace with your actual credentials)\n",
    "REDDIT_CREDENTIALS = {\n",
    "    \"client_id\": \"your_reddit_client_id\",\n",
    "    \"client_secret\": \"your_reddit_client_secret\",\n",
    "    \"username\": \"your_reddit_username\",\n",
    "    \"password\": \"your_reddit_password\"\n",
    "}\n",
    "\n",
    "print(\"üöÄ Reddit Marketing AI Agent - Example Workflow\")\n",
    "print(f\"üìÖ Started at: {datetime.now()}\")\n",
    "print(f\"üåê API Base URL: {API_BASE_URL}\")\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"‚ö†Ô∏è  Reddit Posting: {'ENABLED' if ACTUALLY_POST_TO_REDDIT else 'DISABLED (Safe Mode)'}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def make_request(method: str, endpoint: str, data: Dict = None, params: Dict = None) -> Dict[str, Any]:\n",
    "    \"\"\"Make API request with error handling.\"\"\"\n",
    "    url = f\"{API_BASE_URL}{endpoint}\"\n",
    "    \n",
    "    try:\n",
    "        if method.upper() == \"GET\":\n",
    "            response = requests.get(url, params=params)\n",
    "        elif method.upper() == \"POST\":\n",
    "            response = requests.post(url, json=data, params=params)\n",
    "        elif method.upper() == \"DELETE\":\n",
    "            response = requests.delete(url, params=params)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {method}\")\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "        if hasattr(e.response, 'text'):\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def print_response(title: str, response: Dict[str, Any]):\n",
    "    \"\"\"Pretty print API response.\"\"\"\n",
    "    print(f\"\\nüìã {title}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if \"error\" in response:\n",
    "        print(f\"‚ùå Error: {response['error']}\")\n",
    "        return\n",
    "    \n",
    "    if \"success\" in response:\n",
    "        status = \"‚úÖ\" if response[\"success\"] else \"‚ùå\"\n",
    "        print(f\"{status} Status: {response.get('message', 'No message')}\")\n",
    "    \n",
    "    if \"data\" in response and response[\"data\"]:\n",
    "        print(f\"üìä Data: {json.dumps(response['data'], indent=2)}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API health\n",
    "health_response = make_request(\"GET\", \"/health/\")\n",
    "print_response(\"API Health Check\", health_response)\n",
    "\n",
    "# Check detailed health\n",
    "detailed_health = make_request(\"GET\", \"/health/detailed\")\n",
    "print_response(\"Detailed Health Check\", detailed_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Organization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing organizations\n",
    "orgs_response = make_request(\"GET\", \"/documents/organizations\")\n",
    "print_response(\"Existing Organizations\", orgs_response)\n",
    "\n",
    "# Check if our organization exists\n",
    "org_exists = False\n",
    "if \"data\" in orgs_response and \"organizations\" in orgs_response[\"data\"]:\n",
    "    for org in orgs_response[\"data\"][\"organizations\"]:\n",
    "        if org[\"id\"] == ORGANIZATION_ID:\n",
    "            org_exists = True\n",
    "            print(f\"\\n‚úÖ Organization '{ORGANIZATION_NAME}' already exists\")\n",
    "            break\n",
    "\n",
    "if not org_exists:\n",
    "    print(f\"\\nüìù Organization '{ORGANIZATION_NAME}' will be created during document ingestion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion\n",
    "\n",
    "We'll demonstrate all three document ingestion methods:\n",
    "1. **Direct Content Input** - Paste content directly\n",
    "2. **Simulated File Upload** - Simulate uploading a file\n",
    "3. **URL Scraping** - Scrape content from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Direct Content Input\n",
    "print(\"üìÑ Method 1: Direct Content Input\")\n",
    "\n",
    "direct_documents = [\n",
    "    {\n",
    "        \"title\": \"Python Best Practices Guide\",\n",
    "        \"content\": \"\"\"\n",
    "        Python Best Practices for Clean Code\n",
    "        \n",
    "        Writing clean, maintainable Python code is essential for any developer. Here are some key best practices:\n",
    "        \n",
    "        1. Follow PEP 8 Style Guide\n",
    "        - Use 4 spaces for indentation\n",
    "        - Keep lines under 79 characters\n",
    "        - Use descriptive variable names\n",
    "        \n",
    "        2. Write Docstrings\n",
    "        - Document all functions and classes\n",
    "        - Use triple quotes for docstrings\n",
    "        - Follow Google or NumPy docstring conventions\n",
    "        \n",
    "        3. Use Type Hints\n",
    "        - Add type hints to function parameters and return values\n",
    "        - Use typing module for complex types\n",
    "        - Helps with IDE support and code documentation\n",
    "        \n",
    "        4. Error Handling\n",
    "        - Use specific exception types\n",
    "        - Handle exceptions gracefully\n",
    "        - Log errors appropriately\n",
    "        \n",
    "        5. Testing\n",
    "        - Write unit tests for all functions\n",
    "        - Use pytest for testing framework\n",
    "        - Aim for high test coverage\n",
    "        \n",
    "        These practices will help you write more maintainable and professional Python code.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"intermediate\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Machine Learning Fundamentals\",\n",
    "        \"content\": \"\"\"\n",
    "        Introduction to Machine Learning\n",
    "        \n",
    "        Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\n",
    "        \n",
    "        Types of Machine Learning:\n",
    "        \n",
    "        1. Supervised Learning\n",
    "        - Uses labeled training data\n",
    "        - Examples: Classification, Regression\n",
    "        - Algorithms: Linear Regression, Decision Trees, Random Forest\n",
    "        \n",
    "        2. Unsupervised Learning\n",
    "        - Works with unlabeled data\n",
    "        - Examples: Clustering, Dimensionality Reduction\n",
    "        - Algorithms: K-Means, PCA, DBSCAN\n",
    "        \n",
    "        3. Reinforcement Learning\n",
    "        - Learns through interaction with environment\n",
    "        - Uses rewards and penalties\n",
    "        - Examples: Game playing, Robotics\n",
    "        \n",
    "        Key Concepts:\n",
    "        - Feature Engineering: Selecting and transforming input variables\n",
    "        - Model Training: Teaching the algorithm using training data\n",
    "        - Model Evaluation: Testing performance on unseen data\n",
    "        - Overfitting: When model performs well on training but poorly on new data\n",
    "        \n",
    "        Popular Python Libraries:\n",
    "        - Scikit-learn: General-purpose ML library\n",
    "        - TensorFlow: Deep learning framework\n",
    "        - PyTorch: Research-focused deep learning\n",
    "        - Pandas: Data manipulation and analysis\n",
    "        - NumPy: Numerical computing\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"machine-learning\",\n",
    "            \"difficulty\": \"beginner\",\n",
    "            \"topics\": [\"supervised\", \"unsupervised\", \"reinforcement\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ingest direct content documents\n",
    "direct_response = make_request(\n",
    "    \"POST\", \n",
    "    \"/documents/ingest\",\n",
    "    data=direct_documents,\n",
    "    params={\n",
    "        \"organization_id\": ORGANIZATION_ID,\n",
    "        \"organization_name\": ORGANIZATION_NAME\n",
    "    }\n",
    ")\n",
    "\n",
    "print_response(\"Direct Content Ingestion\", direct_response)\n",
    "\n",
    "# Store document IDs for later use\n",
    "direct_doc_ids = []\n",
    "if \"data\" in direct_response and \"document_ids\" in direct_response[\"data\"]:\n",
    "    direct_doc_ids = direct_response[\"data\"][\"document_ids\"]\n",
    "    print(f\"\\nüìù Stored {len(direct_doc_ids)} document IDs from direct content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Simulated File Upload\n",
    "print(\"\\nüìÅ Method 2: Simulated File Upload\")\n",
    "\n",
    "# Simulate file content (in real scenario, this would be read from an uploaded file)\n",
    "file_content = \"\"\"\n",
    "Web Development with Python and FastAPI\n",
    "\n",
    "FastAPI is a modern, fast web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
    "\n",
    "Key Features:\n",
    "- Fast: Very high performance, on par with NodeJS and Go\n",
    "- Fast to code: Increase the speed to develop features by about 200% to 300%\n",
    "- Fewer bugs: Reduce about 40% of human (developer) induced errors\n",
    "- Intuitive: Great editor support with completion everywhere\n",
    "- Easy: Designed to be easy to use and learn\n",
    "- Short: Minimize code duplication\n",
    "- Robust: Get production-ready code with automatic interactive documentation\n",
    "\n",
    "Getting Started:\n",
    "\n",
    "1. Installation\n",
    "```bash\n",
    "pip install fastapi uvicorn\n",
    "```\n",
    "\n",
    "2. Basic Example\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: str = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "```\n",
    "\n",
    "3. Run the server\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Advanced Features:\n",
    "- Automatic API documentation with Swagger UI\n",
    "- Data validation using Pydantic models\n",
    "- Dependency injection system\n",
    "- Background tasks\n",
    "- WebSocket support\n",
    "- Authentication and authorization\n",
    "- Database integration\n",
    "\n",
    "FastAPI is perfect for building modern web APIs and microservices.\n",
    "\"\"\"\n",
    "\n",
    "# Create document from \"file\" content\n",
    "file_documents = [{\n",
    "    \"title\": \"FastAPI Web Development Guide\",\n",
    "    \"content\": file_content,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"simulated_file_upload\",\n",
    "        \"filename\": \"fastapi_guide.txt\",\n",
    "        \"category\": \"web-development\",\n",
    "        \"framework\": \"fastapi\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "# Ingest file content\n",
    "file_response = make_request(\n",
    "    \"POST\", \n",
    "    \"/documents/ingest\",\n",
    "    data=file_documents,\n",
    "    params={\"organization_id\": ORGANIZATION_ID}\n",
    ")\n",
    "\n",
    "print_response(\"File Content Ingestion\", file_response)\n",
    "\n",
    "# Store document IDs\n",
    "file_doc_ids = []\n",
    "if \"data\" in file_response and \"document_ids\" in file_response[\"data\"]:\n",
    "    file_doc_ids = file_response[\"data\"][\"document_ids\"]\n",
    "    print(f\"\\nüìù Stored {len(file_doc_ids)} document IDs from file content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: URL Scraping\n",
    "print(\"\\nüåê Method 3: URL Scraping\")\n",
    "\n",
    "# Example URLs to scrape (replace with actual URLs you want to scrape)\n",
    "url_requests = [\n",
    "    {\n",
    "        \"url\": \"https://docs.python.org/3/tutorial/introduction.html\",\n",
    "        \"title\": \"Python Tutorial Introduction\",\n",
    "        \"organization_id\": ORGANIZATION_ID,\n",
    "        \"scraping_method\": \"auto\"\n",
    "    }\n",
    "]\n",
    "\n",
    "url_doc_ids = []\n",
    "\n",
    "for url_request in url_requests:\n",
    "    print(f\"\\nüîç Scraping: {url_request['url']}\")\n",
    "    \n",
    "    url_response = make_request(\n",
    "        \"POST\", \n",
    "        \"/documents/ingest-url\",\n",
    "        data=url_request\n",
    "    )\n",
    "    \n",
    "    print_response(f\"URL Scraping: {url_request['title']}\", url_response)\n",
    "    \n",
    "    if \"data\" in url_response and \"document_id\" in url_response[\"data\"]:\n",
    "        url_doc_ids.append(url_response[\"data\"][\"document_id\"])\n",
    "\n",
    "print(f\"\\nüìù Stored {len(url_doc_ids)} document IDs from URL scraping\")\n",
    "\n",
    "# Combine all document IDs\n",
    "all_document_ids = direct_doc_ids + file_doc_ids + url_doc_ids\n",
    "print(f\"\\nüìö Total documents ingested: {len(all_document_ids)}\")\n",
    "print(f\"Document IDs: {all_document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify organization and documents\n",
    "org_docs_response = make_request(\"GET\", f\"/documents/organizations/{ORGANIZATION_ID}\")\n",
    "print_response(\"Organization Documents\", org_docs_response)\n",
    "\n",
    "if \"data\" in org_docs_response and \"organization\" in org_docs_response[\"data\"]:\n",
    "    org_data = org_docs_response[\"data\"][\"organization\"]\n",
    "    print(f\"\\nüìä Organization Summary:\")\n",
    "    print(f\"   Name: {org_data['name']}\")\n",
    "    print(f\"   Documents: {org_data['documents_count']}\")\n",
    "    print(f\"   Created: {org_data['created_at']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Campaign Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new campaign\n",
    "campaign_data = {\n",
    "    \"name\": \"Python Learning Community Outreach 2024\",\n",
    "    \"description\": \"Engage with Python learning communities to share knowledge and best practices\",\n",
    "    \"response_tone\": \"helpful\",\n",
    "    \"max_responses_per_day\": 5\n",
    "}\n",
    "\n",
    "campaign_response = make_request(\n",
    "    \"POST\", \n",
    "    \"/campaigns/\",\n",
    "    data=campaign_data,\n",
    "    params={\"organization_id\": ORGANIZATION_ID}\n",
    ")\n",
    "\n",
    "print_response(\"Campaign Creation\", campaign_response)\n",
    "\n",
    "# Store campaign ID\n",
    "campaign_id = None\n",
    "if \"campaign\" in campaign_response and campaign_response[\"campaign\"]:\n",
    "    campaign_id = campaign_response[\"campaign\"][\"id\"]\n",
    "    print(f\"\\nüéØ Campaign ID: {campaign_id}\")\n",
    "    print(f\"üìù Campaign Name: {campaign_response['campaign']['name']}\")\n",
    "    print(f\"üìä Status: {campaign_response['campaign']['status']}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create campaign\")\n",
    "    campaign_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Discovery\n",
    "\n",
    "First, we'll extract relevant topics from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and all_document_ids:\n",
    "    print(\"üîç Step 1: Discovering Topics from Documents\")\n",
    "    \n",
    "    # Discover topics from selected documents\n",
    "    topic_discovery_data = {\n",
    "        \"document_ids\": all_document_ids[:3]  # Use first 3 documents for topic discovery\n",
    "    }\n",
    "    \n",
    "    topic_response = make_request(\n",
    "        \"POST\", \n",
    "        f\"/campaigns/{campaign_id}/discover-topics\",\n",
    "        data=topic_discovery_data\n",
    "    )\n",
    "    \n",
    "    print_response(\"Topic Discovery\", topic_response)\n",
    "    \n",
    "    # Extract topics for next step\n",
    "    discovered_topics = []\n",
    "    if \"data\" in topic_response and \"topics\" in topic_response[\"data\"]:\n",
    "        discovered_topics = topic_response[\"data\"][\"topics\"]\n",
    "        print(f\"\\nüìã Discovered Topics:\")\n",
    "        for i, topic in enumerate(discovered_topics, 1):\n",
    "            print(f\"   {i}. {topic}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    status_response = make_request(\"GET\", f\"/campaigns/{campaign_id}/status\")\n",
    "    if \"campaign\" in status_response:\n",
    "        print(f\"\\nüìä Campaign Status: {status_response['campaign']['status']}\")\n",
    "        print(f\"üìÑ Documents Selected: {status_response['data']['documents_selected']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or document IDs\")\n",
    "    discovered_topics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subreddit Discovery\n",
    "\n",
    "Now we'll use the discovered topics to find relevant subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and discovered_topics:\n",
    "    print(\"üéØ Step 2: Discovering Subreddits from Topics\")\n",
    "    \n",
    "    # Discover subreddits based on topics\n",
    "    subreddit_discovery_data = {\n",
    "        \"topics\": discovered_topics\n",
    "    }\n",
    "    \n",
    "    subreddit_response = make_request(\n",
    "        \"POST\", \n",
    "        f\"/campaigns/{campaign_id}/discover-subreddits\",\n",
    "        data=subreddit_discovery_data\n",
    "    )\n",
    "    \n",
    "    print_response(\"Subreddit Discovery\", subreddit_response)\n",
    "    \n",
    "    # Extract subreddits for next step\n",
    "    target_subreddits = []\n",
    "    if \"data\" in subreddit_response and \"subreddits\" in subreddit_response[\"data\"]:\n",
    "        target_subreddits = subreddit_response[\"data\"][\"subreddits\"]\n",
    "        print(f\"\\nüéØ Target Subreddits:\")\n",
    "        for i, subreddit in enumerate(target_subreddits, 1):\n",
    "            print(f\"   {i}. r/{subreddit}\")\n",
    "    \n",
    "    # Check updated campaign status\n",
    "    status_response = make_request(\"GET\", f\"/campaigns/{campaign_id}/status\")\n",
    "    if \"campaign\" in status_response:\n",
    "        print(f\"\\nüìä Campaign Status: {status_response['campaign']['status']}\")\n",
    "        print(f\"üéØ Subreddits Found: {status_response['data']['subreddits_found']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or topics\")\n",
    "    target_subreddits = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_subreddits:\n",
    "    print(\"üìù Step 3: Discovering Relevant Posts\")\n",
    "    \n",
    "    # Discover posts in target subreddits\n",
    "    post_discovery_data = {\n",
    "        \"subreddits\": target_subreddits[:3],  # Limit to first 3 subreddits for demo\n",
    "        \"max_posts_per_subreddit\": 5,\n",
    "        \"time_filter\": \"week\",\n",
    "        \"reddit_credentials\": REDDIT_CREDENTIALS\n",
    "    }\n",
    "    \n",
    "    posts_response = make_request(\n",
    "        \"POST\", \n",
    "        f\"/campaigns/{campaign_id}/discover-posts\",\n",
    "        data=post_discovery_data\n",
    "    )\n",
    "    \n",
    "    print_response(\"Post Discovery\", posts_response)\n",
    "    \n",
    "    # Extract post information\n",
    "    target_posts = []\n",
    "    if \"data\" in posts_response and \"posts\" in posts_response[\"data\"]:\n",
    "        target_posts = posts_response[\"data\"][\"posts\"]\n",
    "        print(f\"\\nüìù Found {len(target_posts)} relevant posts:\")\n",
    "        for i, post in enumerate(target_posts[:5], 1):  # Show first 5\n",
    "            print(f\"   {i}. r/{post['subreddit']}: {post['title'][:60]}...\")\n",
    "            print(f\"      Relevance: {post['relevance_score']:.2f} - {post['relevance_reason']}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    status_response = make_request(\"GET\", f\"/campaigns/{campaign_id}/status\")\n",
    "    if \"campaign\" in status_response:\n",
    "        print(f\"\\nüìä Campaign Status: {status_response['campaign']['status']}\")\n",
    "        print(f\"üìù Posts Found: {status_response['data']['posts_found']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or subreddits\")\n",
    "    target_posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_posts:\n",
    "    print(\"üí¨ Step 4: Generating Responses\")\n",
    "    \n",
    "    # Get post IDs for response generation\n",
    "    post_ids = [post[\"id\"] for post in target_posts[:3]]  # Limit to first 3 posts\n",
    "    \n",
    "    response_generation_data = {\n",
    "        \"target_post_ids\": post_ids,\n",
    "        \"tone\": \"helpful\"\n",
    "    }\n",
    "    \n",
    "    generation_response = make_request(\n",
    "        \"POST\", \n",
    "        f\"/campaigns/{campaign_id}/generate-responses\",\n",
    "        data=response_generation_data\n",
    "    )\n",
    "    \n",
    "    print_response(\"Response Generation\", generation_response)\n",
    "    \n",
    "    # Show generated responses\n",
    "    planned_responses = []\n",
    "    if \"data\" in generation_response and \"responses\" in generation_response[\"data\"]:\n",
    "        planned_responses = generation_response[\"data\"][\"responses\"]\n",
    "        print(f\"\\nüí¨ Generated {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"\\n   Response {i}:\")\n",
    "            print(f\"   Target Post: {response['target_post_id']}\")\n",
    "            print(f\"   Confidence: {response['confidence_score']:.2f}\")\n",
    "            print(f\"   Content Preview: {response['response_content'][:100]}...\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    status_response = make_request(\"GET\", f\"/campaigns/{campaign_id}/status\")\n",
    "    if \"campaign\" in status_response:\n",
    "        print(f\"\\nüìä Campaign Status: {status_response['campaign']['status']}\")\n",
    "        print(f\"üí¨ Responses Planned: {status_response['data']['responses_planned']}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or posts\")\n",
    "    planned_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Response Execution (Optional)\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: This step will actually post to Reddit if `ACTUALLY_POST_TO_REDDIT = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and planned_responses:\n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(\"üöÄ Step 5: Executing Responses (POSTING TO REDDIT)\")\n",
    "        \n",
    "        # Get response IDs for execution\n",
    "        response_ids = [response[\"id\"] for response in planned_responses[:2]]  # Limit to first 2\n",
    "        \n",
    "        execution_data = {\n",
    "            \"planned_response_ids\": response_ids,\n",
    "            \"reddit_credentials\": REDDIT_CREDENTIALS\n",
    "        }\n",
    "        \n",
    "        execution_response = make_request(\n",
    "            \"POST\", \n",
    "            f\"/campaigns/{campaign_id}/execute-responses\",\n",
    "            data=execution_data\n",
    "        )\n",
    "        \n",
    "        print_response(\"Response Execution\", execution_response)\n",
    "        \n",
    "        # Show execution results\n",
    "        if \"data\" in execution_response and \"posted_responses\" in execution_response[\"data\"]:\n",
    "            posted_responses = execution_response[\"data\"][\"posted_responses\"]\n",
    "            print(f\"\\nüöÄ Execution Results:\")\n",
    "            for i, response in enumerate(posted_responses, 1):\n",
    "                status = \"‚úÖ Success\" if response[\"posting_successful\"] else \"‚ùå Failed\"\n",
    "                print(f\"   Response {i}: {status}\")\n",
    "                if response[\"posting_successful\"]:\n",
    "                    print(f\"   Reddit URL: https://reddit.com{response['reddit_permalink']}\")\n",
    "                else:\n",
    "                    print(f\"   Error: {response.get('error_message', 'Unknown error')}\")\n",
    "        \n",
    "        # Final campaign status\n",
    "        status_response = make_request(\"GET\", f\"/campaigns/{campaign_id}/status\")\n",
    "        if \"campaign\" in status_response:\n",
    "            print(f\"\\nüìä Final Campaign Status: {status_response['campaign']['status']}\")\n",
    "            print(f\"üöÄ Responses Posted: {status_response['data']['responses_posted']}\")\n",
    "            print(f\"‚úÖ Successful Posts: {status_response['data']['successful_posts']}\")\n",
    "            print(f\"‚ùå Failed Posts: {status_response['data']['failed_posts']}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Step 5: Response Execution SKIPPED (Safe Mode)\")\n",
    "        print(\"\\nüõ°Ô∏è  Reddit posting is disabled for safety.\")\n",
    "        print(\"   To enable posting, set ACTUALLY_POST_TO_REDDIT = True\")\n",
    "        print(\"   and provide valid Reddit credentials.\")\n",
    "        \n",
    "        print(f\"\\nüìã Would have posted {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"   {i}. Response with confidence {response['confidence_score']:.2f}\")\n",
    "            print(f\"      Content: {response['response_content'][:80]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or planned responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analytics & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 6: Analytics & Reporting\")\n",
    "\n",
    "# Get organization quick stats\n",
    "quick_stats_response = make_request(\"GET\", f\"/analytics/organizations/{ORGANIZATION_ID}/quick-stats\")\n",
    "print_response(\"Organization Quick Stats\", quick_stats_response)\n",
    "\n",
    "# Get organization performance report\n",
    "performance_response = make_request(\"GET\", f\"/analytics/organizations/{ORGANIZATION_ID}/performance\")\n",
    "print_response(\"Organization Performance Report\", performance_response)\n",
    "\n",
    "if campaign_id:\n",
    "    # Get campaign engagement report\n",
    "    engagement_response = make_request(\"GET\", f\"/analytics/campaigns/{campaign_id}/engagement\")\n",
    "    print_response(\"Campaign Engagement Report\", engagement_response)\n",
    "\n",
    "# Get platform overview\n",
    "platform_response = make_request(\"GET\", \"/analytics/platform/overview\")\n",
    "print_response(\"Platform Overview\", platform_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary & Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìã Workflow Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"üìö Documents Ingested: {len(all_document_ids)}\")\n",
    "print(f\"   - Direct Content: {len(direct_doc_ids)}\")\n",
    "print(f\"   - File Upload: {len(file_doc_ids)}\")\n",
    "print(f\"   - URL Scraping: {len(url_doc_ids)}\")\n",
    "\n",
    "if campaign_id:\n",
    "    print(f\"\\nüéØ Campaign: {campaign_id}\")\n",
    "    print(f\"üîç Topics Discovered: {len(discovered_topics)}\")\n",
    "    print(f\"üéØ Subreddits Found: {len(target_subreddits)}\")\n",
    "    print(f\"üìù Posts Analyzed: {len(target_posts)}\")\n",
    "    print(f\"üí¨ Responses Generated: {len(planned_responses)}\")\n",
    "    \n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(f\"üöÄ Responses Posted: Executed\")\n",
    "    else:\n",
    "        print(f\"üõ°Ô∏è  Responses Posted: Skipped (Safe Mode)\")\n",
    "\n",
    "print(f\"\\n‚è∞ Completed at: {datetime.now()}\")\n",
    "print(f\"‚úÖ Workflow completed successfully!\")\n",
    "\n",
    "# Display key IDs for reference\n",
    "print(\"\\nüîë Key IDs for Reference:\")\n",
    "print(f\"   Organization ID: {ORGANIZATION_ID}\")\n",
    "if campaign_id:\n",
    "    print(f\"   Campaign ID: {campaign_id}\")\n",
    "if all_document_ids:\n",
    "    print(f\"   Document IDs: {all_document_ids}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ Reddit Marketing AI Agent Workflow Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Operations\n",
    "\n",
    "The following cells demonstrate additional operations you can perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query documents\n",
    "print(\"üîç Document Query Example\")\n",
    "\n",
    "query_data = {\n",
    "    \"query\": \"machine learning algorithms\",\n",
    "    \"organization_id\": ORGANIZATION_ID,\n",
    "    \"method\": \"semantic\",\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "query_response = make_request(\"POST\", \"/documents/query\", data=query_data)\n",
    "print_response(\"Document Query\", query_response)\n",
    "\n",
    "if \"documents\" in query_response:\n",
    "    print(f\"\\nüìÑ Found {len(query_response['documents'])} relevant documents:\")\n",
    "    for i, doc in enumerate(query_response[\"documents\"], 1):\n",
    "        print(f\"   {i}. {doc['title']} (Score: {doc['score']:.3f})\")\n",
    "        print(f\"      Content: {doc['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all campaigns for the organization\n",
    "print(\"üìã List All Campaigns\")\n",
    "\n",
    "campaigns_response = make_request(\n",
    "    \"GET\", \n",
    "    \"/campaigns/\",\n",
    "    params={\"organization_id\": ORGANIZATION_ID}\n",
    ")\n",
    "\n",
    "print_response(\"Organization Campaigns\", campaigns_response)\n",
    "\n",
    "if \"data\" in campaigns_response and \"campaigns\" in campaigns_response[\"data\"]:\n",
    "    campaigns = campaigns_response[\"data\"][\"campaigns\"]\n",
    "    print(f\"\\nüìä Found {len(campaigns)} campaigns:\")\n",
    "    for i, campaign in enumerate(campaigns, 1):\n",
    "        print(f\"   {i}. {campaign['name']} ({campaign['status']})\")\n",
    "        print(f\"      Created: {campaign['created_at']}\")\n",
    "        print(f\"      ID: {campaign['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test subreddit search\n",
    "print(\"üîç Subreddit Search Example\")\n",
    "\n",
    "search_response = make_request(\n",
    "    \"GET\", \n",
    "    \"/subreddits/search\",\n",
    "    params={\"query\": \"python programming\", \"limit\": 5}\n",
    ")\n",
    "\n",
    "print_response(\"Subreddit Search\", search_response)\n",
    "\n",
    "if \"data\" in search_response and \"results\" in search_response[\"data\"]:\n",
    "    results = search_response[\"data\"][\"results\"]\n",
    "    print(f\"\\nüéØ Found {len(results)} subreddits:\")\n",
    "    for i, subreddit in enumerate(results, 1):\n",
    "        print(f\"   {i}. r/{subreddit['name']} ({subreddit['subscribers']:,} subscribers)\")\n",
    "        print(f\"      Description: {subreddit['description'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete workflow of the Reddit Marketing AI Agent:\n",
    "\n",
    "1. ‚úÖ **Setup & Configuration** - Validated environment and API connectivity\n",
    "2. ‚úÖ **Organization Setup** - Created/verified organization\n",
    "3. ‚úÖ **Document Ingestion** - Demonstrated all three ingestion methods\n",
    "4. ‚úÖ **Campaign Creation** - Created a marketing campaign\n",
    "5. ‚úÖ **Topic Discovery** - Extracted relevant topics from documents\n",
    "6. ‚úÖ **Subreddit Discovery** - Found relevant subreddits based on topics\n",
    "7. ‚úÖ **Post Discovery** - Identified relevant posts for engagement\n",
    "8. ‚úÖ **Response Generation** - Generated AI-powered responses\n",
    "9. ‚úÖ **Response Execution** - Demonstrated posting workflow (with safety controls)\n",
    "10. ‚úÖ **Analytics & Reporting** - Generated comprehensive reports\n",
    "\n",
    "### Key Features Highlighted:\n",
    "- **Multiple Document Sources**: Direct content, file upload, and URL scraping\n",
    "- **AI-Powered Analysis**: Topic extraction, subreddit discovery, and response generation\n",
    "- **Safety Controls**: Prevents accidental posting with configurable safety mode\n",
    "- **Comprehensive Analytics**: Detailed reporting and performance tracking\n",
    "- **Modular Workflow**: Each step can be run independently\n",
    "\n",
    "### Next Steps:\n",
    "1. **Configure Real Credentials**: Add your actual Reddit API credentials\n",
    "2. **Enable Posting**: Set `ACTUALLY_POST_TO_REDDIT = True` when ready\n",
    "3. **Customize Content**: Replace example documents with your actual content\n",
    "4. **Monitor Performance**: Use the analytics endpoints to track campaign success\n",
    "5. **Scale Operations**: Create multiple campaigns for different topics/audiences\n",
    "\n",
    "The Reddit Marketing AI Agent is now ready for production use! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}