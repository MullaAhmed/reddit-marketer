{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover Posts and Generate Responses\n",
    "\n",
    "This notebook uses discovered subreddits to find relevant posts, analyze them, generate responses, and post them to Reddit.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load discovered subreddits and select target subreddits\n",
    "2. Search for relevant posts in selected subreddits\n",
    "3. Analyze all discovered posts and generate responses\n",
    "4. Post approved responses to Reddit\n",
    "\n",
    "‚ö†Ô∏è **Warning:** Cell 4 will attempt to post responses to Reddit using your configured credentials!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Load Subreddits\n",
    "\n",
    "Initialize services and load the previously discovered subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.storage.json_storage import JsonStorage\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "\n",
    "# Load discovered subreddits from previous notebook\n",
    "print(\"Loading discovered subreddits...\")\n",
    "try:\n",
    "    with open('discovered_subreddits_output.json', 'r') as f:\n",
    "        subreddits_data = json.load(f)\n",
    "    \n",
    "    ranked_subreddits = subreddits_data['ranked_subreddits']\n",
    "    organization_id = subreddits_data['organization_id']\n",
    "    \n",
    "    if not subreddits_data['discovery_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Subreddit discovery failed in previous notebook\")\n",
    "        print(f\"Error: {subreddits_data.get('error_message', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(f\"üìã Loaded {len(ranked_subreddits)} discovered subreddits:\")\n",
    "        for i, subreddit in enumerate(ranked_subreddits, 1):\n",
    "            print(f\"  {i}. r/{subreddit}\")\n",
    "        \n",
    "        # Select top subreddits to focus on (limit to 2-3 for manageable processing)\n",
    "        selected_subreddits = ranked_subreddits[:3]  # Top 3 subreddits\n",
    "        \n",
    "        print(f\"\\nüéØ Selected {len(selected_subreddits)} subreddits for post discovery:\")\n",
    "        for i, subreddit in enumerate(selected_subreddits, 1):\n",
    "            print(f\"  {i}. r/{subreddit}\")\n",
    "        \n",
    "        # Save selected subreddits\n",
    "        selected_subreddits_data = {\n",
    "            \"selected_subreddits\": selected_subreddits,\n",
    "            \"organization_id\": organization_id,\n",
    "            \"all_discovered_subreddits\": ranked_subreddits,\n",
    "            \"selection_success\": True\n",
    "        }\n",
    "        \n",
    "        with open('selected_subreddits.json', 'w') as f:\n",
    "            json.dump(selected_subreddits_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìÅ Saved selected subreddits to selected_subreddits.json\")\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: discovered_subreddits_output.json not found.\")\n",
    "    print(\"Please run the ingest_extract_discover.ipynb notebook first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading subreddits: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Discover Posts\n",
    "\n",
    "Search for relevant posts within the selected subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.clients.reddit_client import RedditClient\n",
    "\n",
    "# Initialize Reddit client\n",
    "reddit_client = RedditClient(\n",
    "    client_id=settings.REDDIT_CLIENT_ID,\n",
    "    client_secret=settings.REDDIT_CLIENT_SECRET,\n",
    "    username=settings.REDDIT_USERNAME,\n",
    "    password=settings.REDDIT_PASSWORD\n",
    ")\n",
    "\n",
    "# Load selected subreddits from previous cell\n",
    "print(\"Loading selected subreddits...\")\n",
    "try:\n",
    "    with open('selected_subreddits.json', 'r') as f:\n",
    "        selected_data = json.load(f)\n",
    "    \n",
    "    selected_subreddits = selected_data['selected_subreddits']\n",
    "    organization_id = selected_data['organization_id']\n",
    "    \n",
    "    if not selected_data['selection_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Subreddit selection failed\")\n",
    "    else:\n",
    "        print(f\"üéØ Target subreddits: {', '.join([f'r/{s}' for s in selected_subreddits])}\")\n",
    "        \n",
    "        # Define search queries related to our AI marketing platform\n",
    "        search_queries = [\n",
    "            \"AI marketing\",\n",
    "            \"marketing automation\",\n",
    "            \"social media management\",\n",
    "            \"content generation\",\n",
    "            \"marketing tools\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüîç Search queries: {', '.join(search_queries)}\")\n",
    "        \n",
    "        # Discover posts from selected subreddits\n",
    "        all_discovered_posts = []\n",
    "        \n",
    "        async with reddit_client:\n",
    "            for subreddit in selected_subreddits:\n",
    "                print(f\"\\nüì° Searching r/{subreddit}...\")\n",
    "                \n",
    "                for query in search_queries:\n",
    "                    try:\n",
    "                        posts = await reddit_client.search_subreddit_posts(\n",
    "                            subreddit=subreddit,\n",
    "                            query=query,\n",
    "                            sort=\"relevance\",\n",
    "                            time_filter=\"month\",  # Posts from last month\n",
    "                            limit=3  # Limit per query to manage volume\n",
    "                        )\n",
    "                        \n",
    "                        for post in posts:\n",
    "                            # Add search context to post data\n",
    "                            post['search_query'] = query\n",
    "                            post['source_subreddit'] = subreddit\n",
    "                            all_discovered_posts.append(post)\n",
    "                        \n",
    "                        print(f\"  Found {len(posts)} posts for '{query}'\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ö†Ô∏è Error searching '{query}' in r/{subreddit}: {str(e)}\")\n",
    "        \n",
    "        # Remove duplicates based on post ID\n",
    "        unique_posts = {}\n",
    "        for post in all_discovered_posts:\n",
    "            post_id = post['id']\n",
    "            if post_id not in unique_posts:\n",
    "                unique_posts[post_id] = post\n",
    "        \n",
    "        discovered_posts = list(unique_posts.values())\n",
    "        \n",
    "        # Limit total posts to process (top 10 most relevant)\n",
    "        discovered_posts = discovered_posts[:10]\n",
    "        \n",
    "        print(f\"\\n‚úÖ Discovery complete!\")\n",
    "        print(f\"üìä Found {len(discovered_posts)} unique posts to analyze:\")\n",
    "        \n",
    "        for i, post in enumerate(discovered_posts, 1):\n",
    "            print(f\"  {i}. [{post['source_subreddit']}] {post['title'][:60]}...\")\n",
    "            print(f\"     Score: {post['score']}, Comments: {post['num_comments']}\")\n",
    "        \n",
    "        # Save discovered posts\n",
    "        discovered_posts_data = {\n",
    "            \"discovered_posts\": discovered_posts,\n",
    "            \"organization_id\": organization_id,\n",
    "            \"search_queries\": search_queries,\n",
    "            \"target_subreddits\": selected_subreddits,\n",
    "            \"total_posts_found\": len(discovered_posts),\n",
    "            \"discovery_success\": True\n",
    "        }\n",
    "        \n",
    "        with open('discovered_posts_output.json', 'w') as f:\n",
    "            json.dump(discovered_posts_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìÅ Saved discovered posts to discovered_posts_output.json\")\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: selected_subreddits.json not found. Please run Cell 1 first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error discovering posts: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Analyze Posts and Generate Responses\n",
    "\n",
    "Iterate through all discovered posts, analyze each, and generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.clients.reddit_client import RedditClient\n",
    "from src.clients.llm_client import LLMClient\n",
    "from src.storage.vector_storage import VectorStorage\n",
    "from src.storage.json_storage import JsonStorage\n",
    "from src.services.posting_service import PostingService\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage = VectorStorage()\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=settings.REDDIT_CLIENT_ID,\n",
    "    client_secret=settings.REDDIT_CLIENT_SECRET,\n",
    "    username=settings.REDDIT_USERNAME,\n",
    "    password=settings.REDDIT_PASSWORD\n",
    ")\n",
    "posting_service = PostingService(reddit_client, llm_client, vector_storage, json_storage)\n",
    "\n",
    "# Load discovered posts from previous cell\n",
    "print(\"Loading discovered posts...\")\n",
    "try:\n",
    "    with open('discovered_posts_output.json', 'r') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    discovered_posts = posts_data['discovered_posts']\n",
    "    organization_id = posts_data['organization_id']\n",
    "    \n",
    "    if not posts_data['discovery_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Post discovery failed in previous cell\")\n",
    "    else:\n",
    "        print(f\"üìã Loaded {len(discovered_posts)} posts to analyze\")\n",
    "        \n",
    "        # Analyze each post and generate responses\n",
    "        all_generated_responses = []\n",
    "        successful_analyses = 0\n",
    "        failed_analyses = 0\n",
    "        \n",
    "        print(f\"\\nü§ñ Starting analysis and response generation...\")\n",
    "        \n",
    "        for i, post_data in enumerate(discovered_posts, 1):\n",
    "            post_id = post_data['id']\n",
    "            post_title = post_data['title']\n",
    "            subreddit = post_data['source_subreddit']\n",
    "            \n",
    "            print(f\"\\nüìù [{i}/{len(discovered_posts)}] Analyzing post in r/{subreddit}:\")\n",
    "            print(f\"    Title: {post_title[:80]}...\")\n",
    "            print(f\"    Post ID: {post_id}\")\n",
    "            \n",
    "            try:\n",
    "                # Analyze post and generate response\n",
    "                success, message, response_data = await posting_service.analyze_and_generate_response(\n",
    "                    post_id=post_id,\n",
    "                    organization_id=organization_id,\n",
    "                    tone=\"helpful\"\n",
    "                )\n",
    "                \n",
    "                if success and response_data:\n",
    "                    # Add metadata to response\n",
    "                    response_data['source_post'] = {\n",
    "                        'title': post_title,\n",
    "                        'subreddit': subreddit,\n",
    "                        'score': post_data.get('score', 0),\n",
    "                        'num_comments': post_data.get('num_comments', 0)\n",
    "                    }\n",
    "                    \n",
    "                    all_generated_responses.append(response_data)\n",
    "                    successful_analyses += 1\n",
    "                    \n",
    "                    print(f\"    ‚úÖ Response generated successfully\")\n",
    "                    print(f\"    Target: {response_data['target']['response_type']}\")\n",
    "                    print(f\"    Confidence: {response_data['response']['confidence']:.2f}\")\n",
    "                    print(f\"    Preview: {response_data['response']['content'][:100]}...\")\n",
    "                else:\n",
    "                    failed_analyses += 1\n",
    "                    print(f\"    ‚ùå Analysis failed: {message}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_analyses += 1\n",
    "                print(f\"    ‚ùå Error analyzing post: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Analysis Summary:\")\n",
    "        print(f\"  ‚úÖ Successful: {successful_analyses}\")\n",
    "        print(f\"  ‚ùå Failed: {failed_analyses}\")\n",
    "        print(f\"  üìù Total responses generated: {len(all_generated_responses)}\")\n",
    "        \n",
    "        # Save generated responses\n",
    "        generated_responses_data = {\n",
    "            \"generated_responses\": all_generated_responses,\n",
    "            \"organization_id\": organization_id,\n",
    "            \"total_posts_analyzed\": len(discovered_posts),\n",
    "            \"successful_analyses\": successful_analyses,\n",
    "            \"failed_analyses\": failed_analyses,\n",
    "            \"responses_ready_to_post\": len(all_generated_responses),\n",
    "            \"generation_success\": True\n",
    "        }\n",
    "        \n",
    "        with open('generated_responses_output.json', 'w') as f:\n",
    "            json.dump(generated_responses_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìÅ Saved generated responses to generated_responses_output.json\")\n",
    "        \n",
    "        if len(all_generated_responses) > 0:\n",
    "            print(f\"\\nüöÄ Ready to post {len(all_generated_responses)} responses in the next cell!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è No responses were generated. Check the analysis results above.\")\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: discovered_posts_output.json not found. Please run Cell 2 first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error during analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Post Responses\n",
    "\n",
    "‚ö†Ô∏è **WARNING: This cell will post responses to Reddit using your configured credentials!**\n",
    "\n",
    "Make sure you have reviewed the generated responses and are ready to post them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.clients.reddit_client import RedditClient\n",
    "from src.storage.json_storage import JsonStorage\n",
    "from src.services.posting_service import PostingService\n",
    "from src.clients.llm_client import LLMClient\n",
    "from src.storage.vector_storage import VectorStorage\n",
    "\n",
    "print(\"‚ö†Ô∏è  WARNING: This cell will post responses to Reddit!\")\n",
    "print(\"‚ö†Ô∏è  Make sure you have reviewed the generated responses.\")\n",
    "print(\"‚ö†Ô∏è  Proceeding will use your Reddit account credentials.\\n\")\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage = VectorStorage()\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=settings.REDDIT_CLIENT_ID,\n",
    "    client_secret=settings.REDDIT_CLIENT_SECRET,\n",
    "    username=settings.REDDIT_USERNAME,\n",
    "    password=settings.REDDIT_PASSWORD\n",
    ")\n",
    "posting_service = PostingService(reddit_client, llm_client, vector_storage, json_storage)\n",
    "\n",
    "# Load generated responses from previous cell\n",
    "print(\"Loading generated responses...\")\n",
    "try:\n",
    "    with open('generated_responses_output.json', 'r') as f:\n",
    "        responses_data = json.load(f)\n",
    "    \n",
    "    generated_responses = responses_data['generated_responses']\n",
    "    organization_id = responses_data['organization_id']\n",
    "    \n",
    "    if not responses_data['generation_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Response generation failed in previous cell\")\n",
    "    elif len(generated_responses) == 0:\n",
    "        print(f\"‚ùå No responses to post. Please check the previous cell.\")\n",
    "    else:\n",
    "        print(f\"üìã Loaded {len(generated_responses)} responses ready to post\")\n",
    "        \n",
    "        # Show preview of responses to be posted\n",
    "        print(f\"\\nüìù Preview of responses to be posted:\")\n",
    "        for i, response_data in enumerate(generated_responses, 1):\n",
    "            target = response_data['target']\n",
    "            response = response_data['response']\n",
    "            source_post = response_data.get('source_post', {})\n",
    "            \n",
    "            print(f\"\\n  {i}. Post: {source_post.get('title', 'Unknown')[:50]}...\")\n",
    "            print(f\"     Subreddit: r/{response_data.get('subreddit', 'unknown')}\")\n",
    "            print(f\"     Action: {target['response_type']}\")\n",
    "            print(f\"     Confidence: {response['confidence']:.2f}\")\n",
    "            print(f\"     Response preview: {response['content'][:100]}...\")\n",
    "        \n",
    "        # Post all responses\n",
    "        print(f\"\\nüöÄ Starting to post {len(generated_responses)} responses...\")\n",
    "        \n",
    "        all_posting_results = []\n",
    "        successful_posts = 0\n",
    "        failed_posts = 0\n",
    "        \n",
    "        for i, response_data in enumerate(generated_responses, 1):\n",
    "            target = response_data['target']\n",
    "            response = response_data['response']\n",
    "            source_post = response_data.get('source_post', {})\n",
    "            \n",
    "            target_id = target['target_id']\n",
    "            response_type = target['response_type']\n",
    "            response_content = response['content']\n",
    "            \n",
    "            print(f\"\\nüì§ [{i}/{len(generated_responses)}] Posting to r/{response_data.get('subreddit', 'unknown')}...\")\n",
    "            print(f\"    Post: {source_post.get('title', 'Unknown')[:50]}...\")\n",
    "            print(f\"    Action: {response_type}\")\n",
    "            \n",
    "            try:\n",
    "                # Post the response\n",
    "                success, message, result = await posting_service.post_approved_response(\n",
    "                    response_type=response_type,\n",
    "                    response_content=response_content,\n",
    "                    target_id=target_id\n",
    "                )\n",
    "                \n",
    "                posting_result = {\n",
    "                    \"response_index\": i,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"response_type\": response_type,\n",
    "                    \"success\": success,\n",
    "                    \"message\": message,\n",
    "                    \"result\": result,\n",
    "                    \"source_post\": source_post\n",
    "                }\n",
    "                \n",
    "                all_posting_results.append(posting_result)\n",
    "                \n",
    "                if success:\n",
    "                    successful_posts += 1\n",
    "                    print(f\"    ‚úÖ Posted successfully!\")\n",
    "                    if result and 'permalink' in result:\n",
    "                        print(f\"    üîó Link: https://reddit.com{result['permalink']}\")\n",
    "                else:\n",
    "                    failed_posts += 1\n",
    "                    print(f\"    ‚ùå Posting failed: {message}\")\n",
    "                \n",
    "                # Add delay between posts to respect rate limits\n",
    "                if i < len(generated_responses):\n",
    "                    print(f\"    ‚è≥ Waiting 10 seconds before next post...\")\n",
    "                    await asyncio.sleep(10)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                failed_posts += 1\n",
    "                posting_result = {\n",
    "                    \"response_index\": i,\n",
    "                    \"target_id\": target_id,\n",
    "                    \"response_type\": response_type,\n",
    "                    \"success\": False,\n",
    "                    \"error\": str(e),\n",
    "                    \"source_post\": source_post\n",
    "                }\n",
    "                all_posting_results.append(posting_result)\n",
    "                print(f\"    ‚ùå Error posting: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nüìä Posting Summary:\")\n",
    "        print(f\"  ‚úÖ Successful posts: {successful_posts}\")\n",
    "        print(f\"  ‚ùå Failed posts: {failed_posts}\")\n",
    "        print(f\"  üìù Total attempts: {len(generated_responses)}\")\n",
    "        \n",
    "        # Save posting results\n",
    "        posted_responses_data = {\n",
    "            \"all_posting_results\": all_posting_results,\n",
    "            \"organization_id\": organization_id,\n",
    "            \"total_attempts\": len(generated_responses),\n",
    "            \"successful_posts\": successful_posts,\n",
    "            \"failed_posts\": failed_posts,\n",
    "            \"posting_complete\": True\n",
    "        }\n",
    "        \n",
    "        with open('posted_responses_results.json', 'w') as f:\n",
    "            json.dump(posted_responses_data, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nüìÅ Saved posting results to posted_responses_results.json\")\n",
    "        \n",
    "        if successful_posts > 0:\n",
    "            print(f\"\\nüéâ Successfully posted {successful_posts} responses to Reddit!\")\n",
    "            print(f\"üìà You can now run the analytics notebook to track engagement.\")\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: generated_responses_output.json not found. Please run Cell 3 first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error during posting: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}