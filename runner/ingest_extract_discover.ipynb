{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Ingestion, Topic Extraction, and Subreddit Discovery\n",
    "\n",
    "This notebook handles the complete workflow from document ingestion to subreddit discovery using the Reddit Marketing AI Agent.\n",
    "\n",
    "**Workflow:**\n",
    "1. Ingest document content\n",
    "2. Extract relevant topics from the document\n",
    "3. Discover and rank subreddits based on topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup and Document Ingestion\n",
    "\n",
    "Initialize services and ingest a document with directly provided content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.storage.json_storage import JsonStorage\n",
    "from src.storage.vector_storage import VectorStorage\n",
    "from src.services.ingestion_service import IngestionService\n",
    "from src.models.common import generate_id\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage = VectorStorage()\n",
    "ingestion_service = IngestionService(json_storage, vector_storage)\n",
    "\n",
    "# Define organization ID (consistent across all notebooks)\n",
    "organization_id = \"my_marketing_org\"\n",
    "\n",
    "# Define document content and title\n",
    "content = \"\"\"\n",
    "We are a cutting-edge AI-powered marketing automation platform that helps businesses \n",
    "optimize their social media presence and engagement. Our platform uses advanced \n",
    "machine learning algorithms to analyze customer behavior, predict trends, and \n",
    "automatically generate personalized content for various social media channels.\n",
    "\n",
    "Key features include:\n",
    "- Automated content generation using GPT models\n",
    "- Real-time sentiment analysis and engagement tracking\n",
    "- Multi-platform social media management (Twitter, LinkedIn, Reddit, Facebook)\n",
    "- Advanced analytics and ROI tracking\n",
    "- AI-driven audience segmentation and targeting\n",
    "- Automated A/B testing for content optimization\n",
    "\n",
    "Our target audience includes:\n",
    "- Small to medium businesses looking to scale their marketing efforts\n",
    "- Marketing agencies seeking automation tools\n",
    "- E-commerce businesses wanting to increase online presence\n",
    "- SaaS companies looking to improve customer acquisition\n",
    "- Content creators and influencers seeking efficiency tools\n",
    "\n",
    "We specialize in helping businesses increase their organic reach, improve engagement \n",
    "rates, and convert social media followers into paying customers through intelligent \n",
    "automation and data-driven insights.\n",
    "\"\"\"\n",
    "\n",
    "title = \"AI Marketing Automation Platform - Company Overview\"\n",
    "\n",
    "# Ingest the document\n",
    "print(\"Starting document ingestion...\")\n",
    "success, message, document_id = await ingestion_service.ingest_document(\n",
    "    content=content,\n",
    "    title=title,\n",
    "    organization_id=organization_id,\n",
    "    is_url=False\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úÖ Document ingestion successful!\")\n",
    "    print(f\"Document ID: {document_id}\")\n",
    "    print(f\"Message: {message}\")\n",
    "    \n",
    "    # Save document info to JSON file\n",
    "    ingested_docs_data = {\n",
    "        \"document_id\": document_id,\n",
    "        \"organization_id\": organization_id,\n",
    "        \"title\": title,\n",
    "        \"content_length\": len(content),\n",
    "        \"ingestion_success\": True\n",
    "    }\n",
    "    \n",
    "    with open('ingested_docs.json', 'w') as f:\n",
    "        json.dump(ingested_docs_data, f, indent=2)\n",
    "    \n",
    "    print(f\"üìÅ Saved document info to ingested_docs.json\")\n",
    "else:\n",
    "    print(f\"‚ùå Document ingestion failed: {message}\")\n",
    "    # Save failure info\n",
    "    ingested_docs_data = {\n",
    "        \"document_id\": None,\n",
    "        \"organization_id\": organization_id,\n",
    "        \"title\": title,\n",
    "        \"ingestion_success\": False,\n",
    "        \"error_message\": message\n",
    "    }\n",
    "    \n",
    "    with open('ingested_docs.json', 'w') as f:\n",
    "        json.dump(ingested_docs_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Extract Topics\n",
    "\n",
    "Extract relevant topics from the ingested document using the document_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.storage.json_storage import JsonStorage\n",
    "from src.storage.vector_storage import VectorStorage\n",
    "from src.clients.llm_client import LLMClient\n",
    "from src.clients.reddit_client import RedditClient\n",
    "from src.services.subreddit_service import SubredditService\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage = VectorStorage()\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=settings.REDDIT_CLIENT_ID,\n",
    "    client_secret=settings.REDDIT_CLIENT_SECRET,\n",
    "    username=settings.REDDIT_USERNAME,\n",
    "    password=settings.REDDIT_PASSWORD\n",
    ")\n",
    "subreddit_service = SubredditService(reddit_client, llm_client, vector_storage, json_storage)\n",
    "\n",
    "# Load document info from previous cell\n",
    "print(\"Loading document information...\")\n",
    "try:\n",
    "    with open('ingested_docs.json', 'r') as f:\n",
    "        ingested_data = json.load(f)\n",
    "    \n",
    "    document_id = ingested_data['document_id']\n",
    "    organization_id = ingested_data['organization_id']\n",
    "    \n",
    "    if not ingested_data['ingestion_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Document ingestion failed in previous cell\")\n",
    "        print(f\"Error: {ingested_data.get('error_message', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(f\"üìÑ Document ID: {document_id}\")\n",
    "        print(f\"üè¢ Organization ID: {organization_id}\")\n",
    "        \n",
    "        # Extract topics from the document\n",
    "        print(\"\\nExtracting topics from document...\")\n",
    "        success, message, topics = await subreddit_service.extract_topics_from_documents(\n",
    "            organization_id=organization_id,\n",
    "            document_ids=[document_id]\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úÖ Topic extraction successful!\")\n",
    "            print(f\"Extracted {len(topics)} topics:\")\n",
    "            for i, topic in enumerate(topics, 1):\n",
    "                print(f\"  {i}. {topic}\")\n",
    "            \n",
    "            # Save topics to JSON file\n",
    "            extracted_topics_data = {\n",
    "                \"topics\": topics,\n",
    "                \"organization_id\": organization_id,\n",
    "                \"document_id\": document_id,\n",
    "                \"extraction_success\": True,\n",
    "                \"topics_count\": len(topics)\n",
    "            }\n",
    "            \n",
    "            with open('extracted_topics_output.json', 'w') as f:\n",
    "                json.dump(extracted_topics_data, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nüìÅ Saved topics to extracted_topics_output.json\")\n",
    "        else:\n",
    "            print(f\"‚ùå Topic extraction failed: {message}\")\n",
    "            # Save failure info\n",
    "            extracted_topics_data = {\n",
    "                \"topics\": [],\n",
    "                \"organization_id\": organization_id,\n",
    "                \"document_id\": document_id,\n",
    "                \"extraction_success\": False,\n",
    "                \"error_message\": message\n",
    "            }\n",
    "            \n",
    "            with open('extracted_topics_output.json', 'w') as f:\n",
    "                json.dump(extracted_topics_data, f, indent=2)\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: ingested_docs.json not found. Please run Cell 1 first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading document info: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Discover Subreddits\n",
    "\n",
    "Discover and rank subreddits based on the extracted topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the system path to allow importing from src\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from src.config.settings import settings\n",
    "from src.storage.json_storage import JsonStorage\n",
    "from src.storage.vector_storage import VectorStorage\n",
    "from src.clients.llm_client import LLMClient\n",
    "from src.clients.reddit_client import RedditClient\n",
    "from src.services.subreddit_service import SubredditService\n",
    "\n",
    "# Initialize services\n",
    "json_storage = JsonStorage()\n",
    "vector_storage = VectorStorage()\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=settings.REDDIT_CLIENT_ID,\n",
    "    client_secret=settings.REDDIT_CLIENT_SECRET,\n",
    "    username=settings.REDDIT_USERNAME,\n",
    "    password=settings.REDDIT_PASSWORD\n",
    ")\n",
    "subreddit_service = SubredditService(reddit_client, llm_client, vector_storage, json_storage)\n",
    "\n",
    "# Load topics from previous cell\n",
    "print(\"Loading extracted topics...\")\n",
    "try:\n",
    "    with open('extracted_topics_output.json', 'r') as f:\n",
    "        topics_data = json.load(f)\n",
    "    \n",
    "    topics = topics_data['topics']\n",
    "    organization_id = topics_data['organization_id']\n",
    "    \n",
    "    if not topics_data['extraction_success']:\n",
    "        print(f\"‚ùå Cannot proceed: Topic extraction failed in previous cell\")\n",
    "        print(f\"Error: {topics_data.get('error_message', 'Unknown error')}\")\n",
    "    else:\n",
    "        print(f\"üìã Loaded {len(topics)} topics:\")\n",
    "        for i, topic in enumerate(topics, 1):\n",
    "            print(f\"  {i}. {topic}\")\n",
    "        print(f\"üè¢ Organization ID: {organization_id}\")\n",
    "        \n",
    "        # Discover and rank subreddits\n",
    "        print(\"\\nDiscovering subreddits based on topics...\")\n",
    "        success, message, ranked_subreddits = await subreddit_service.discover_and_rank_subreddits(\n",
    "            topics=topics,\n",
    "            organization_id=organization_id,\n",
    "            use_rag_context=True\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(f\"‚úÖ Subreddit discovery successful!\")\n",
    "            print(f\"Found and ranked {len(ranked_subreddits)} subreddits:\")\n",
    "            for i, subreddit in enumerate(ranked_subreddits, 1):\n",
    "                print(f\"  {i}. r/{subreddit}\")\n",
    "            \n",
    "            # Save subreddits to JSON file\n",
    "            discovered_subreddits_data = {\n",
    "                \"ranked_subreddits\": ranked_subreddits,\n",
    "                \"organization_id\": organization_id,\n",
    "                \"source_topics\": topics,\n",
    "                \"discovery_success\": True,\n",
    "                \"subreddits_count\": len(ranked_subreddits),\n",
    "                \"discovery_message\": message\n",
    "            }\n",
    "            \n",
    "            with open('discovered_subreddits_output.json', 'w') as f:\n",
    "                json.dump(discovered_subreddits_data, f, indent=2)\n",
    "            \n",
    "            print(f\"\\nüìÅ Saved subreddits to discovered_subreddits_output.json\")\n",
    "        else:\n",
    "            print(f\"‚ùå Subreddit discovery failed: {message}\")\n",
    "            # Save failure info\n",
    "            discovered_subreddits_data = {\n",
    "                \"ranked_subreddits\": [],\n",
    "                \"organization_id\": organization_id,\n",
    "                \"source_topics\": topics,\n",
    "                \"discovery_success\": False,\n",
    "                \"error_message\": message\n",
    "            }\n",
    "            \n",
    "            with open('discovered_subreddits_output.json', 'w') as f:\n",
    "                json.dump(discovered_subreddits_data, f, indent=2)\n",
    "\nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: extracted_topics_output.json not found. Please run Cell 2 first.\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error loading topics: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}