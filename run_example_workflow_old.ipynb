{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Marketing AI Agent - Complete Example Workflow (Direct Functions)\n",
    "\n",
    "This notebook demonstrates the complete workflow of the Reddit Marketing AI Agent using direct function calls instead of API endpoints.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Setup & Configuration** - Environment validation and service initialization\n",
    "2. **Organization Setup** - Create and configure an organization\n",
    "3. **Document Ingestion** - Multiple methods (direct content, file upload, URL scraping)\n",
    "4. **Campaign Creation** - Create and configure a marketing campaign\n",
    "5. **Topic Discovery** - Extract relevant topics from documents\n",
    "6. **Subreddit Discovery** - Find relevant subreddits based on topics\n",
    "7. **Post Discovery** - Find relevant posts in target subreddits\n",
    "8. **Response Generation** - AI-generated contextual responses\n",
    "9. **Response Execution** - Post responses to Reddit (with safety controls)\n",
    "10. **Analytics & Reporting** - Comprehensive performance analysis\n",
    "\n",
    "## Safety Features:\n",
    "- **Reddit Posting Control**: `ACTUALLY_POST_TO_REDDIT = False` prevents accidental posting\n",
    "- **Credential Validation**: Checks for required API keys\n",
    "- **Error Handling**: Graceful handling of service failures\n",
    "- **Independent Cells**: Each step can be run independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Reddit Marketing AI Agent - Example Workflow (Direct Functions)\n",
      "üìÖ Started at: 2025-06-22 06:48:24.961111\n",
      "üè¢ Organization: Example Organization (example-org-2024)\n",
      "‚ö†Ô∏è  Reddit Posting: ENABLED\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import  Any\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('app')\n",
    "\n",
    "# Import all necessary services and managers\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.services.analytics_service import AnalyticsService\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.analytics_manager import AnalyticsManager\n",
    "\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "\n",
    "from app.models.campaign import (\n",
    "    CampaignCreateRequest, SubredditDiscoveryRequest, SubredditDiscoveryByTopicsRequest,\n",
    "    PostDiscoveryRequest, ResponseGenerationRequest, ResponseExecutionRequest,\n",
    "    ResponseTone\n",
    ")\n",
    "from app.models.document import DocumentCreateRequest, DocumentIngestURLRequest, DocumentQuery\n",
    "\n",
    "from app.core.settings import settings\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"example-org-2024\"\n",
    "ORGANIZATION_NAME = \"Example Organization\"\n",
    "\n",
    "# Safety control - Set to True only when you want to actually post to Reddit\n",
    "ACTUALLY_POST_TO_REDDIT = True\n",
    "\n",
    "# Reddit credentials (replace with your actual credentials)\n",
    "REDDIT_CREDENTIALS = {\n",
    "     \"client_id\": os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    \"client_secret\": os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    \"username\": os.getenv(\"REDDIT_USERNAME\"),\n",
    "    \"password\": os.getenv(\"REDDIT_PASSWORD\")\n",
    "}\n",
    "\n",
    "print(\"üöÄ Reddit Marketing AI Agent - Example Workflow (Direct Functions)\")\n",
    "print(f\"üìÖ Started at: {datetime.now()}\")\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"‚ö†Ô∏è  Reddit Posting: {'ENABLED' if ACTUALLY_POST_TO_REDDIT else 'DISABLED (Safe Mode)'}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing Services and Managers...\n",
      "‚úÖ All services initialized successfully!\n",
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize all services and managers\n",
    "print(\"üîß Initializing Services and Managers...\")\n",
    "\n",
    "# Storage layer\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "\n",
    "# Managers\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "analytics_manager = AnalyticsManager(campaign_manager, document_manager)\n",
    "\n",
    "# Clients\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"],\n",
    "    username=REDDIT_CREDENTIALS.get(\"username\"),\n",
    "    password=REDDIT_CREDENTIALS.get(\"password\")\n",
    ")\n",
    "web_scraper_service = WebScraperService()\n",
    "\n",
    "# Services\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "analytics_service = AnalyticsService(analytics_manager)\n",
    "\n",
    "print(\"‚úÖ All services initialized successfully!\")\n",
    "\n",
    "# Helper functions\n",
    "def print_result(title: str, success: bool, message: str, data: Any = None):\n",
    "    \"\"\"Pretty print service results.\"\"\"\n",
    "    print(f\"\\nüìã {title}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "    print(f\"{status} Status: {message}\")\n",
    "    \n",
    "    if data:\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                print(f\"üìä {key}: {value}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(f\"üìä Items: {len(data)}\")\n",
    "            for i, item in enumerate(data[:3], 1):  # Show first 3 items\n",
    "                print(f\"   {i}. {str(item)[:80]}...\")\n",
    "        else:\n",
    "            print(f\"üìä Data: {str(data)[:100]}...\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Check\n",
      "----------------------------------------\n",
      "Required API Keys:\n",
      "   ‚úÖ OPENAI_API_KEY: Set\n",
      "   ‚úÖ GOOGLE_API_KEY: Set\n",
      "\n",
      "Optional API Keys:\n",
      "   ‚úÖ GROQ_API_KEY: Set\n",
      "   ‚úÖ FIRECRAWL_API_KEY: Set\n",
      "   ‚ö†Ô∏è LANGCHAIN_PROJECT: Not set\n",
      "\n",
      "üìÅ Data Directory: data\n",
      "ü§ñ Default Model: gpt-4o\n",
      "üîç Embedding Provider: openai\n"
     ]
    }
   ],
   "source": [
    "# Check environment and settings\n",
    "print(\"üîç Environment Check\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "required_keys = {\n",
    "    \"OPENAI_API_KEY\": settings.OPENAI_API_KEY,\n",
    "    \"GOOGLE_API_KEY\": settings.GOOGLE_API_KEY\n",
    "}\n",
    "\n",
    "optional_keys = {\n",
    "    \"GROQ_API_KEY\": settings.GROQ_API_KEY,\n",
    "    \"FIRECRAWL_API_KEY\": settings.FIRECRAWL_API_KEY,\n",
    "    \"LANGCHAIN_PROJECT\": settings.LANGCHAIN_PROJECT\n",
    "}\n",
    "\n",
    "print(\"Required API Keys:\")\n",
    "for key, value in required_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Missing'}\")\n",
    "\n",
    "print(\"\\nOptional API Keys:\")\n",
    "for key, value in optional_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Not set'}\")\n",
    "\n",
    "print(f\"\\nüìÅ Data Directory: {settings.DATA_DIR}\")\n",
    "print(f\"ü§ñ Default Model: {settings.MODEL_NAME}\")\n",
    "print(f\"üîç Embedding Provider: {settings.EMBEDDING_PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Organization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¢ Setting up Organization\n",
      "‚úÖ Organization: Example Organization\n",
      "üìä ID: example-org-2024\n",
      "üìÑ Documents: 0\n",
      "üìÖ Created: 2025-06-22 01:18:25.001847+00:00\n",
      "üîÑ Active: True\n",
      "\n",
      "üìã Total organizations in system: 1\n",
      "   - Example Organization (example-org-2024): 0 documents\n"
     ]
    }
   ],
   "source": [
    "# Get or create organization\n",
    "print(\"üè¢ Setting up Organization\")\n",
    "\n",
    "organization = document_service.get_or_create_organization(ORGANIZATION_ID, ORGANIZATION_NAME)\n",
    "\n",
    "print(f\"‚úÖ Organization: {organization.name}\")\n",
    "print(f\"üìä ID: {organization.id}\")\n",
    "print(f\"üìÑ Documents: {organization.documents_count}\")\n",
    "print(f\"üìÖ Created: {organization.created_at}\")\n",
    "print(f\"üîÑ Active: {organization.is_active}\")\n",
    "\n",
    "# List all organizations\n",
    "all_organizations = document_service.list_organizations()\n",
    "print(f\"\\nüìã Total organizations in system: {len(all_organizations)}\")\n",
    "for org in all_organizations:\n",
    "    print(f\"   - {org.name} ({org.id}): {org.documents_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion\n",
    "\n",
    "We'll demonstrate all three document ingestion methods:\n",
    "1. **Direct Content Input** - Paste content directly\n",
    "2. **Simulated File Upload** - Simulate uploading a file\n",
    "3. **URL Scraping** - Scrape content from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Method 1: Direct Content Input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:01,  1.53s/it]\n",
      "Calculating embeddings: 1it [00:00,  1.37it/s]\n",
      "Document 81c80b87-d6bc-48e8-829a-90b217530e1a_chunk_0 contains `meta` values of unsupported types for the keys: topics. These items will be discarded. Supported types are: str, int, float, bool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Direct Content Ingestion\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested 2 documents (2 chunks)\n",
      "üìä document_ids: ['5f2fa40a-b25a-4765-a2cc-8d98e84e1636', '81c80b87-d6bc-48e8-829a-90b217530e1a']\n",
      "üìä documents_ingested: 2\n",
      "\n",
      "üìù Stored 2 document IDs from direct content\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Direct Content Input\n",
    "print(\"üìÑ Method 1: Direct Content Input\")\n",
    "\n",
    "direct_documents = [\n",
    "    {\n",
    "        \"title\": \"Python Best Practices Guide\",\n",
    "        \"content\": \"\"\"\n",
    "        Python Best Practices for Clean Code\n",
    "        \n",
    "        Writing clean, maintainable Python code is essential for any developer. Here are some key best practices:\n",
    "        \n",
    "        1. Follow PEP 8 Style Guide\n",
    "        - Use 4 spaces for indentation\n",
    "        - Keep lines under 79 characters\n",
    "        - Use descriptive variable names\n",
    "        \n",
    "        2. Write Docstrings\n",
    "        - Document all functions and classes\n",
    "        - Use triple quotes for docstrings\n",
    "        - Follow Google or NumPy docstring conventions\n",
    "        \n",
    "        3. Use Type Hints\n",
    "        - Add type hints to function parameters and return values\n",
    "        - Use typing module for complex types\n",
    "        - Helps with IDE support and code documentation\n",
    "        \n",
    "        4. Error Handling\n",
    "        - Use specific exception types\n",
    "        - Handle exceptions gracefully\n",
    "        - Log errors appropriately\n",
    "        \n",
    "        5. Testing\n",
    "        - Write unit tests for all functions\n",
    "        - Use pytest for testing framework\n",
    "        - Aim for high test coverage\n",
    "        \n",
    "        These practices will help you write more maintainable and professional Python code.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"intermediate\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Machine Learning Fundamentals\",\n",
    "        \"content\": \"\"\"\n",
    "        Introduction to Machine Learning\n",
    "        \n",
    "        Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\n",
    "        \n",
    "        Types of Machine Learning:\n",
    "        \n",
    "        1. Supervised Learning\n",
    "        - Uses labeled training data\n",
    "        - Examples: Classification, Regression\n",
    "        - Algorithms: Linear Regression, Decision Trees, Random Forest\n",
    "        \n",
    "        2. Unsupervised Learning\n",
    "        - Works with unlabeled data\n",
    "        - Examples: Clustering, Dimensionality Reduction\n",
    "        - Algorithms: K-Means, PCA, DBSCAN\n",
    "        \n",
    "        3. Reinforcement Learning\n",
    "        - Learns through interaction with environment\n",
    "        - Uses rewards and penalties\n",
    "        - Examples: Game playing, Robotics\n",
    "        \n",
    "        Key Concepts:\n",
    "        - Feature Engineering: Selecting and transforming input variables\n",
    "        - Model Training: Teaching the algorithm using training data\n",
    "        - Model Evaluation: Testing performance on unseen data\n",
    "        - Overfitting: When model performs well on training but poorly on new data\n",
    "        \n",
    "        Popular Python Libraries:\n",
    "        - Scikit-learn: General-purpose ML library\n",
    "        - TensorFlow: Deep learning framework\n",
    "        - PyTorch: Research-focused deep learning\n",
    "        - Pandas: Data manipulation and analysis\n",
    "        - NumPy: Numerical computing\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"machine-learning\",\n",
    "            \"difficulty\": \"beginner\",\n",
    "            \"topics\": [\"supervised\", \"unsupervised\", \"reinforcement\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ingest direct content documents\n",
    "success, message, direct_doc_ids = document_service.ingest_documents(\n",
    "    documents=direct_documents,\n",
    "    org_id=ORGANIZATION_ID,\n",
    "    org_name=ORGANIZATION_NAME\n",
    ")\n",
    "\n",
    "print_result(\"Direct Content Ingestion\", success, message, {\n",
    "    \"document_ids\": direct_doc_ids,\n",
    "    \"documents_ingested\": len(direct_doc_ids) if direct_doc_ids else 0\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Stored {len(direct_doc_ids) if direct_doc_ids else 0} document IDs from direct content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Method 2: Simulated File Upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã File Content Ingestion\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested 1 documents (1 chunks)\n",
      "üìä document_ids: ['3da9bbe3-18df-4616-a424-66d0f0c9aa16']\n",
      "üìä documents_ingested: 1\n",
      "\n",
      "üìù Stored 1 document IDs from file content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Simulated File Upload\n",
    "print(\"\\nüìÅ Method 2: Simulated File Upload\")\n",
    "\n",
    "# Simulate file content (in real scenario, this would be read from an uploaded file)\n",
    "file_content = \"\"\"\n",
    "Web Development with Python and FastAPI\n",
    "\n",
    "FastAPI is a modern, fast web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
    "\n",
    "Key Features:\n",
    "- Fast: Very high performance, on par with NodeJS and Go\n",
    "- Fast to code: Increase the speed to develop features by about 200% to 300%\n",
    "- Fewer bugs: Reduce about 40% of human (developer) induced errors\n",
    "- Intuitive: Great editor support with completion everywhere\n",
    "- Easy: Designed to be easy to use and learn\n",
    "- Short: Minimize code duplication\n",
    "- Robust: Get production-ready code with automatic interactive documentation\n",
    "\n",
    "Getting Started:\n",
    "\n",
    "1. Installation\n",
    "```bash\n",
    "pip install fastapi uvicorn\n",
    "```\n",
    "\n",
    "2. Basic Example\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: str = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "```\n",
    "\n",
    "3. Run the server\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Advanced Features:\n",
    "- Automatic API documentation with Swagger UI\n",
    "- Data validation using Pydantic models\n",
    "- Dependency injection system\n",
    "- Background tasks\n",
    "- WebSocket support\n",
    "- Authentication and authorization\n",
    "- Database integration\n",
    "\n",
    "FastAPI is perfect for building modern web APIs and microservices.\n",
    "\"\"\"\n",
    "\n",
    "# Create document from \"file\" content\n",
    "file_documents = [{\n",
    "    \"title\": \"FastAPI Web Development Guide\",\n",
    "    \"content\": file_content,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"simulated_file_upload\",\n",
    "        \"filename\": \"fastapi_guide.txt\",\n",
    "        \"category\": \"web-development\",\n",
    "        \"framework\": \"fastapi\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "# Ingest file content\n",
    "success, message, file_doc_ids = document_service.ingest_documents(\n",
    "    documents=file_documents,\n",
    "    org_id=ORGANIZATION_ID\n",
    ")\n",
    "\n",
    "print_result(\"File Content Ingestion\", success, message, {\n",
    "    \"document_ids\": file_doc_ids,\n",
    "    \"documents_ingested\": len(file_doc_ids) if file_doc_ids else 0\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Stored {len(file_doc_ids) if file_doc_ids else 0} document IDs from file content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Method 3: URL Scraping\n",
      "\n",
      "üîç Scraping: https://docs.python.org/3/tutorial/introduction.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:01,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã URL Scraping: Python Tutorial Introduction\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested document from URL: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä document_id: 4b9ad109-a7ca-4743-aafe-55cd454dd504\n",
      "üìä url: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä scraping_method: auto\n",
      "\n",
      "üìù Stored 1 document IDs from URL scraping\n",
      "\n",
      "üìö Total documents ingested: 4\n",
      "Document IDs: ['5f2fa40a-b25a-4765-a2cc-8d98e84e1636', '81c80b87-d6bc-48e8-829a-90b217530e1a', '3da9bbe3-18df-4616-a424-66d0f0c9aa16', '4b9ad109-a7ca-4743-aafe-55cd454dd504']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: URL Scraping\n",
    "print(\"\\nüåê Method 3: URL Scraping\")\n",
    "\n",
    "# Example URLs to scrape (replace with actual URLs you want to scrape)\n",
    "url_requests = [\n",
    "    {\n",
    "        \"url\": \"https://docs.python.org/3/tutorial/introduction.html\",\n",
    "        \"title\": \"Python Tutorial Introduction\",\n",
    "        \"scraping_method\": \"auto\"\n",
    "    }\n",
    "]\n",
    "\n",
    "url_doc_ids = []\n",
    "\n",
    "for url_request in url_requests:\n",
    "    print(f\"\\nüîç Scraping: {url_request['url']}\")\n",
    "    \n",
    "    # Use async function with asyncio\n",
    "    success, message, document_id = await document_service.ingest_document_from_url(\n",
    "        url=url_request[\"url\"],\n",
    "        organization_id=ORGANIZATION_ID,\n",
    "        title=url_request[\"title\"],\n",
    "        scraping_method=url_request[\"scraping_method\"]\n",
    "    )\n",
    "    \n",
    "    print_result(f\"URL Scraping: {url_request['title']}\", success, message, {\n",
    "        \"document_id\": document_id,\n",
    "        \"url\": url_request[\"url\"],\n",
    "        \"scraping_method\": url_request[\"scraping_method\"]\n",
    "    })\n",
    "    \n",
    "    if success and document_id:\n",
    "        url_doc_ids.append(document_id)\n",
    "\n",
    "print(f\"\\nüìù Stored {len(url_doc_ids)} document IDs from URL scraping\")\n",
    "\n",
    "# Combine all document IDs\n",
    "all_document_ids = (direct_doc_ids or []) + (file_doc_ids or []) + url_doc_ids\n",
    "print(f\"\\nüìö Total documents ingested: {len(all_document_ids)}\")\n",
    "print(f\"Document IDs: {all_document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Organization Summary:\n",
      "   Name: Example Organization\n",
      "   Documents: 4\n",
      "   Created: 2025-06-22 01:18:25.001847+00:00\n",
      "\n",
      "üìà Organization Statistics:\n",
      "   Total Documents: 4\n",
      "   Total Chunks: 17\n",
      "   Total Content Length: 25,377 characters\n",
      "   Average Chunks per Document: 4.2\n"
     ]
    }
   ],
   "source": [
    "# Verify organization and documents\n",
    "updated_organization = document_service.get_or_create_organization(ORGANIZATION_ID)\n",
    "\n",
    "print(f\"üìä Organization Summary:\")\n",
    "print(f\"   Name: {updated_organization.name}\")\n",
    "print(f\"   Documents: {updated_organization.documents_count}\")\n",
    "print(f\"   Created: {updated_organization.created_at}\")\n",
    "\n",
    "# Get organization stats\n",
    "org_stats = document_service.get_organization_stats(ORGANIZATION_ID)\n",
    "if \"error\" not in org_stats:\n",
    "    print(f\"\\nüìà Organization Statistics:\")\n",
    "    print(f\"   Total Documents: {org_stats['total_documents']}\")\n",
    "    print(f\"   Total Chunks: {org_stats['total_chunks']}\")\n",
    "    print(f\"   Total Content Length: {org_stats['total_content_length']:,} characters\")\n",
    "    print(f\"   Average Chunks per Document: {org_stats['average_chunks_per_document']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Campaign Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Campaign Creation\n",
      "----------------------------------------\n",
      "‚úÖ Status: Campaign 'Python Learning Community Outreach 2024' created successfully\n",
      "\n",
      "üéØ Campaign ID: 9ce6f9b0-ab6f-46cb-9ed1-490bc57ef950\n",
      "üìù Campaign Name: Python Learning Community Outreach 2024\n",
      "üìä Status: CampaignStatus.CREATED\n",
      "üéµ Tone: ResponseTone.HELPFUL\n",
      "üìÖ Created: 2025-06-22 01:18:34.876664+00:00\n"
     ]
    }
   ],
   "source": [
    "# Create a new campaign\n",
    "campaign_request = CampaignCreateRequest(\n",
    "    name=\"Python Learning Community Outreach 2024\",\n",
    "    description=\"Engage with Python learning communities to share knowledge and best practices\",\n",
    "    response_tone=ResponseTone.HELPFUL,\n",
    "    max_responses_per_day=5\n",
    ")\n",
    "\n",
    "success, message, campaign = await campaign_service.create_campaign(\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    request=campaign_request\n",
    ")\n",
    "\n",
    "print_result(\"Campaign Creation\", success, message)\n",
    "\n",
    "# Store campaign for later use\n",
    "campaign_id = None\n",
    "if success and campaign:\n",
    "    campaign_id = campaign.id\n",
    "    print(f\"\\nüéØ Campaign ID: {campaign_id}\")\n",
    "    print(f\"üìù Campaign Name: {campaign.name}\")\n",
    "    print(f\"üìä Status: {campaign.status}\")\n",
    "    print(f\"üéµ Tone: {campaign.response_tone}\")\n",
    "    print(f\"üìÖ Created: {campaign.created_at}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create campaign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Discovery\n",
    "\n",
    "First, we'll extract relevant topics from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 1: Discovering Topics from Documents\n",
      "\n",
      "üìã Topic Discovery\n",
      "----------------------------------------\n",
      "‚úÖ Status: Extracted 20 topics from 3 documents\n",
      "üìä topics: ['Python', 'Clean Code', 'PEP 8', 'Docstrings', 'Type Hints', 'Error Handling', 'Testing', 'Machine Learning', 'Supervised Learning', 'Unsupervised Learning', 'Reinforcement Learning', 'Scikit-learn', 'TensorFlow', 'PyTorch', 'Pandas', 'NumPy', 'Web Development', 'FastAPI', 'API', 'Microservices']\n",
      "üìä selected_document_ids: ['5f2fa40a-b25a-4765-a2cc-8d98e84e1636', '81c80b87-d6bc-48e8-829a-90b217530e1a', '3da9bbe3-18df-4616-a424-66d0f0c9aa16']\n",
      "üìä total_topics: 20\n",
      "\n",
      "üìã Discovered Topics:\n",
      "   1. Python\n",
      "   2. Clean Code\n",
      "   3. PEP 8\n",
      "   4. Docstrings\n",
      "   5. Type Hints\n",
      "   6. Error Handling\n",
      "   7. Testing\n",
      "   8. Machine Learning\n",
      "   9. Supervised Learning\n",
      "   10. Unsupervised Learning\n",
      "   11. Reinforcement Learning\n",
      "   12. Scikit-learn\n",
      "   13. TensorFlow\n",
      "   14. PyTorch\n",
      "   15. Pandas\n",
      "   16. NumPy\n",
      "   17. Web Development\n",
      "   18. FastAPI\n",
      "   19. API\n",
      "   20. Microservices\n",
      "\n",
      "üìä Campaign Status: CampaignStatus.DOCUMENTS_UPLOADED\n",
      "üìÑ Documents Selected: 3\n"
     ]
    }
   ],
   "source": [
    "if campaign_id and all_document_ids:\n",
    "    print(\"üîç Step 1: Discovering Topics from Documents\")\n",
    "    \n",
    "    # Discover topics from selected documents\n",
    "    topic_discovery_request = SubredditDiscoveryRequest(\n",
    "        document_ids=all_document_ids[:3]  # Use first 3 documents for topic discovery\n",
    "    )\n",
    "    \n",
    "    success, message, topic_data = await campaign_service.discover_topics(\n",
    "        campaign_id=campaign_id,\n",
    "        request=topic_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Topic Discovery\", success, message, topic_data)\n",
    "    \n",
    "    # Extract topics for next step\n",
    "    discovered_topics = []\n",
    "    if success and topic_data and \"topics\" in topic_data:\n",
    "        discovered_topics = topic_data[\"topics\"]\n",
    "        print(f\"\\nüìã Discovered Topics:\")\n",
    "        for i, topic in enumerate(discovered_topics, 1):\n",
    "            print(f\"   {i}. {topic}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üìÑ Documents Selected: {len(updated_campaign.selected_document_ids)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or document IDs\")\n",
    "    discovered_topics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subreddit Discovery\n",
    "\n",
    "Now we'll use the discovered topics to find relevant subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Step 2: Discovering Subreddits from Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error searching subreddits for 'PyTorch': error with request \n",
      "Error searching subreddits for topic 'PyTorch': error with request \n",
      "Error searching subreddits for 'Testing': error with request \n",
      "Error searching subreddits for topic 'Testing': error with request \n",
      "Error searching subreddits for 'NumPy': error with request \n",
      "Error searching subreddits for topic 'NumPy': error with request \n",
      "Error searching subreddits for 'FastAPI': error with request \n",
      "Error searching subreddits for topic 'FastAPI': error with request \n",
      "Error searching subreddits for 'Clean Code': error with request \n",
      "Error searching subreddits for topic 'Clean Code': error with request \n",
      "Error searching subreddits for 'Scikit-learn': error with request \n",
      "Error searching subreddits for topic 'Scikit-learn': error with request \n",
      "Error searching subreddits for 'Pandas': error with request \n",
      "Error searching subreddits for topic 'Pandas': error with request \n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/errorhandling: received 403 HTTP response\n",
      "Error getting details for r/errorhandling: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/api: received 403 HTTP response\n",
      "Error getting details for r/api: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Discover subreddits based on topics\u001b[39;00m\n\u001b[32m      5\u001b[39m subreddit_discovery_request = SubredditDiscoveryByTopicsRequest(\n\u001b[32m      6\u001b[39m     topics=discovered_topics\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m success, message, subreddit_data = \u001b[38;5;28;01mawait\u001b[39;00m campaign_service.discover_subreddits(\n\u001b[32m     10\u001b[39m     campaign_id=campaign_id,\n\u001b[32m     11\u001b[39m     request=subreddit_discovery_request\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m print_result(\u001b[33m\"\u001b[39m\u001b[33mSubreddit Discovery\u001b[39m\u001b[33m\"\u001b[39m, success, message, subreddit_data)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Extract subreddits for next step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\services\\campaign_service.py:185\u001b[39m, in \u001b[36mCampaignService.discover_subreddits\u001b[39m\u001b[34m(self, campaign_id, request)\u001b[39m\n\u001b[32m    179\u001b[39m campaign_context = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_relevant_campaign_context(\n\u001b[32m    180\u001b[39m     campaign.organization_id, \n\u001b[32m    181\u001b[39m     campaign.selected_document_ids\n\u001b[32m    182\u001b[39m )\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Discover subreddits using Reddit service with provided topics\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m success, message, discovery_data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reddit_service.discover_subreddits_by_topics(\n\u001b[32m    186\u001b[39m     topics=request.topics,\n\u001b[32m    187\u001b[39m     organization_id=campaign.organization_id,\n\u001b[32m    188\u001b[39m     min_subscribers=\u001b[32m10000\u001b[39m\n\u001b[32m    189\u001b[39m )\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m success:\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, message, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\services\\reddit_service.py:75\u001b[39m, in \u001b[36mRedditService.discover_subreddits_by_topics\u001b[39m\u001b[34m(self, topics, organization_id, min_subscribers)\u001b[39m\n\u001b[32m     67\u001b[39m tasks = [\n\u001b[32m     68\u001b[39m     asyncio.create_task(\n\u001b[32m     69\u001b[39m         \u001b[38;5;28mself\u001b[39m._search_subreddits_by_topic(topic, \u001b[38;5;28mself\u001b[39m._reddit_client)\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m topics\n\u001b[32m     72\u001b[39m ]\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Run all search tasks concurrently\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks, return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m topic, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(topics, results):\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;167;01mException\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\services\\reddit_service.py:122\u001b[39m, in \u001b[36mRedditService._search_subreddits_by_topic\u001b[39m\u001b[34m(self, topic, reddit_client)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m subreddit_name, details \u001b[38;5;129;01min\u001b[39;00m search_results.items():\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    121\u001b[39m         \u001b[38;5;66;03m# Use RedditClient's get_subreddit_info\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m         info = \u001b[38;5;28;01mawait\u001b[39;00m reddit_client.get_subreddit_info(subreddit_name)\n\u001b[32m    123\u001b[39m         subreddit_dict[subreddit_name] = {\n\u001b[32m    124\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mabout\u001b[39m\u001b[33m\"\u001b[39m: info.get(\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    125\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msubscribers\u001b[39m\u001b[33m\"\u001b[39m: info.get(\u001b[33m\"\u001b[39m\u001b[33msubscribers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m)\n\u001b[32m    126\u001b[39m         }\n\u001b[32m    127\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\clients\\reddit_client.py:276\u001b[39m, in \u001b[36mRedditClient.get_subreddit_info\u001b[39m\u001b[34m(self, subreddit_name)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._initialize_reddit()\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     subreddit = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._execute_with_retry(\n\u001b[32m    277\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._reddit_instance.subreddit(subreddit_name, fetch=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    278\u001b[39m     )\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: subreddit.display_name,\n\u001b[32m    282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msubscribers\u001b[39m\u001b[33m\"\u001b[39m: subreddit.subscribers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: subreddit.url\n\u001b[32m    287\u001b[39m     }\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\clients\\reddit_client.py:119\u001b[39m, in \u001b[36mRedditClient._execute_with_retry\u001b[39m\u001b[34m(self, coroutine_func, *args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m retry \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_retries + \u001b[32m1\u001b[39m):\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enforce_rate_limits()\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coroutine_func(*args, **kwargs)\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RedditAPIException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\Marketer\\Reddit\\app\\clients\\reddit_client.py:107\u001b[39m, in \u001b[36mRedditClient._enforce_rate_limits\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait_time > \u001b[32m0\u001b[39m:\n\u001b[32m    106\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRate limit reached. Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(wait_time)\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Add random delay to avoid bursts\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(random.uniform(\u001b[32m0.1\u001b[39m, \u001b[32m0.5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:639\u001b[39m, in \u001b[36msleep\u001b[39m\u001b[34m(delay, result)\u001b[39m\n\u001b[32m    635\u001b[39m h = loop.call_later(delay,\n\u001b[32m    636\u001b[39m                     futures._set_result_unless_cancelled,\n\u001b[32m    637\u001b[39m                     future, result)\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    641\u001b[39m     h.cancel()\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if campaign_id and discovered_topics[:3]:\n",
    "    print(\"üéØ Step 2: Discovering Subreddits from Topics\")\n",
    "    \n",
    "    # Discover subreddits based on topics\n",
    "    subreddit_discovery_request = SubredditDiscoveryByTopicsRequest(\n",
    "        topics=discovered_topics\n",
    "    )\n",
    "    \n",
    "    success, message, subreddit_data = await campaign_service.discover_subreddits(\n",
    "        campaign_id=campaign_id,\n",
    "        request=subreddit_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Subreddit Discovery\", success, message, subreddit_data)\n",
    "    \n",
    "    # Extract subreddits for next step\n",
    "    target_subreddits = []\n",
    "    if success and subreddit_data and \"subreddits\" in subreddit_data:\n",
    "        target_subreddits = subreddit_data[\"subreddits\"]\n",
    "        print(f\"\\nüéØ Target Subreddits:\")\n",
    "        for i, subreddit in enumerate(target_subreddits, 1):\n",
    "            print(f\"   {i}. r/{subreddit}\")\n",
    "    \n",
    "    # Check updated campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üéØ Subreddits Found: {len(updated_campaign.target_subreddits)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or topics\")\n",
    "    target_subreddits = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_subreddits:\n",
    "    print(\"üìù Step 3: Discovering Relevant Posts\")\n",
    "    \n",
    "    # Discover posts in target subreddits\n",
    "    post_discovery_request = PostDiscoveryRequest(\n",
    "        subreddits=target_subreddits[:3],  # Limit to first 3 subreddits for demo\n",
    "        max_posts_per_subreddit=5,\n",
    "        time_filter=\"week\",\n",
    "        reddit_credentials=REDDIT_CREDENTIALS\n",
    "    )\n",
    "    \n",
    "    success, message, posts_data = await campaign_service.discover_posts(\n",
    "        campaign_id=campaign_id,\n",
    "        request=post_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Post Discovery\", success, message, posts_data)\n",
    "    \n",
    "    # Extract post information\n",
    "    target_posts = []\n",
    "    if success and posts_data and \"posts\" in posts_data:\n",
    "        target_posts = posts_data[\"posts\"]\n",
    "        print(f\"\\nüìù Found {len(target_posts)} relevant posts:\")\n",
    "        for i, post in enumerate(target_posts[:5], 1):  # Show first 5\n",
    "            print(f\"   {i}. r/{post['subreddit']}: {post['title'][:60]}...\")\n",
    "            print(f\"      Relevance: {post['relevance_score']:.2f} - {post['relevance_reason']}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üìù Posts Found: {len(updated_campaign.target_posts)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or subreddits\")\n",
    "    target_posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_posts:\n",
    "    print(\"üí¨ Step 4: Generating Responses\")\n",
    "    \n",
    "    # Get post IDs for response generation\n",
    "    post_ids = [post[\"id\"] for post in target_posts[:3]]  # Limit to first 3 posts\n",
    "    \n",
    "    response_generation_request = ResponseGenerationRequest(\n",
    "        target_post_ids=post_ids,\n",
    "        tone=ResponseTone.HELPFUL\n",
    "    )\n",
    "    \n",
    "    success, message, generation_data = await campaign_service.generate_responses(\n",
    "        campaign_id=campaign_id,\n",
    "        request=response_generation_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Response Generation\", success, message, generation_data)\n",
    "    \n",
    "    # Show generated responses\n",
    "    planned_responses = []\n",
    "    if success and generation_data and \"responses\" in generation_data:\n",
    "        planned_responses = generation_data[\"responses\"]\n",
    "        print(f\"\\nüí¨ Generated {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"\\n   Response {i}:\")\n",
    "            print(f\"   Target Post: {response['target_post_id']}\")\n",
    "            print(f\"   Confidence: {response['confidence_score']:.2f}\")\n",
    "            print(f\"   Content Preview: {response['response_content'][:100]}...\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üí¨ Responses Planned: {len(updated_campaign.planned_responses)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or posts\")\n",
    "    planned_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Response Execution (Optional)\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: This step will actually post to Reddit if `ACTUALLY_POST_TO_REDDIT = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and planned_responses:\n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(\"üöÄ Step 5: Executing Responses (POSTING TO REDDIT)\")\n",
    "        \n",
    "        # Get response IDs for execution\n",
    "        response_ids = [response[\"id\"] for response in planned_responses[:2]]  # Limit to first 2\n",
    "        \n",
    "        execution_request = ResponseExecutionRequest(\n",
    "            planned_response_ids=response_ids,\n",
    "            reddit_credentials=REDDIT_CREDENTIALS\n",
    "        )\n",
    "        \n",
    "        success, message, execution_data = await campaign_service.execute_responses(\n",
    "            campaign_id=campaign_id,\n",
    "            request=execution_request\n",
    "        )\n",
    "        \n",
    "        print_result(\"Response Execution\", success, message, execution_data)\n",
    "        \n",
    "        # Show execution results\n",
    "        if success and execution_data and \"posted_responses\" in execution_data:\n",
    "            posted_responses = execution_data[\"posted_responses\"]\n",
    "            print(f\"\\nüöÄ Execution Results:\")\n",
    "            for i, response in enumerate(posted_responses, 1):\n",
    "                status = \"‚úÖ Success\" if response[\"posting_successful\"] else \"‚ùå Failed\"\n",
    "                print(f\"   Response {i}: {status}\")\n",
    "                if response[\"posting_successful\"]:\n",
    "                    print(f\"   Reddit URL: https://reddit.com{response['reddit_permalink']}\")\n",
    "                else:\n",
    "                    print(f\"   Error: {response.get('error_message', 'Unknown error')}\")\n",
    "        \n",
    "        # Final campaign status\n",
    "        success_status, message_status, final_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "        if success_status and final_campaign:\n",
    "            print(f\"\\nüìä Final Campaign Status: {final_campaign.status}\")\n",
    "            print(f\"üöÄ Responses Posted: {len(final_campaign.posted_responses)}\")\n",
    "            successful_posts = len([r for r in final_campaign.posted_responses.values() if r.posting_successful])\n",
    "            failed_posts = len([r for r in final_campaign.posted_responses.values() if not r.posting_successful])\n",
    "            print(f\"‚úÖ Successful Posts: {successful_posts}\")\n",
    "            print(f\"‚ùå Failed Posts: {failed_posts}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Step 5: Response Execution SKIPPED (Safe Mode)\")\n",
    "        print(\"\\nüõ°Ô∏è  Reddit posting is disabled for safety.\")\n",
    "        print(\"   To enable posting, set ACTUALLY_POST_TO_REDDIT = True\")\n",
    "        print(\"   and provide valid Reddit credentials.\")\n",
    "        \n",
    "        print(f\"\\nüìã Would have posted {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"   {i}. Response with confidence {response['confidence_score']:.2f}\")\n",
    "            print(f\"      Content: {response['response_content'][:80]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or planned responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analytics & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 6: Analytics & Reporting\")\n",
    "\n",
    "# Get organization quick stats\n",
    "quick_stats = analytics_service.get_quick_stats(ORGANIZATION_ID)\n",
    "print_result(\"Organization Quick Stats\", \"error\" not in quick_stats, \"Quick stats retrieved\", quick_stats)\n",
    "\n",
    "# Get organization performance report\n",
    "performance_report = analytics_service.get_organization_performance_report(ORGANIZATION_ID)\n",
    "print_result(\"Organization Performance Report\", \"error\" not in performance_report, \"Performance report generated\", {\n",
    "    \"report_type\": performance_report.get(\"report_type\"),\n",
    "    \"insights_count\": len(performance_report.get(\"performance_insights\", [])),\n",
    "    \"campaign_stats\": performance_report.get(\"campaign_stats\", {})\n",
    "})\n",
    "\n",
    "if campaign_id:\n",
    "    # Get campaign engagement report\n",
    "    engagement_report = analytics_service.get_campaign_engagement_report(campaign_id)\n",
    "    print_result(\"Campaign Engagement Report\", \"error\" not in engagement_report, \"Engagement report generated\", {\n",
    "        \"campaign_name\": engagement_report.get(\"campaign_name\"),\n",
    "        \"status\": engagement_report.get(\"status\"),\n",
    "        \"basic_stats\": engagement_report.get(\"basic_stats\", {})\n",
    "    })\n",
    "\n",
    "# Get platform overview\n",
    "platform_overview = analytics_service.get_overall_platform_metrics()\n",
    "print_result(\"Platform Overview\", \"error\" not in platform_overview, \"Platform overview generated\", {\n",
    "    \"total_campaigns\": platform_overview.get(\"campaign_stats\", {}).get(\"total_campaigns\", 0),\n",
    "    \"total_organizations\": platform_overview.get(\"campaign_stats\", {}).get(\"total_organizations\", 0),\n",
    "    \"insights_count\": len(platform_overview.get(\"platform_insights\", []))\n",
    "})\n",
    "\n",
    "# Get subreddit effectiveness report\n",
    "subreddit_effectiveness = analytics_service.get_subreddit_effectiveness_report(ORGANIZATION_ID)\n",
    "print_result(\"Subreddit Effectiveness\", \"error\" not in subreddit_effectiveness, \"Subreddit effectiveness analyzed\", {\n",
    "    \"total_subreddits\": subreddit_effectiveness.get(\"total_subreddits_analyzed\", 0),\n",
    "    \"recommendations_count\": len(subreddit_effectiveness.get(\"recommendations\", []))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query documents\n",
    "print(\"üîç Document Query Example\")\n",
    "\n",
    "query = DocumentQuery(\n",
    "    query=\"machine learning algorithms\",\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    method=\"semantic\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "query_response = document_service.query_documents(query)\n",
    "\n",
    "print_result(\"Document Query\", True, f\"Found {query_response.total_results} documents\", {\n",
    "    \"query\": query_response.query,\n",
    "    \"method\": query_response.method,\n",
    "    \"processing_time_ms\": query_response.processing_time_ms,\n",
    "    \"total_results\": query_response.total_results\n",
    "})\n",
    "\n",
    "if query_response.documents:\n",
    "    print(f\"\\nüìÑ Found {len(query_response.documents)} relevant documents:\")\n",
    "    for i, doc in enumerate(query_response.documents, 1):\n",
    "        print(f\"   {i}. {doc.title} (Score: {doc.score:.3f})\")\n",
    "        print(f\"      Content: {doc.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all campaigns for the organization\n",
    "print(\"üìã List All Campaigns\")\n",
    "\n",
    "success, message, campaigns = await campaign_service.list_campaigns(ORGANIZATION_ID)\n",
    "\n",
    "print_result(\"Organization Campaigns\", success, message, {\n",
    "    \"campaigns_count\": len(campaigns) if campaigns else 0\n",
    "})\n",
    "\n",
    "if success and campaigns:\n",
    "    print(f\"\\nüìä Found {len(campaigns)} campaigns:\")\n",
    "    for i, campaign in enumerate(campaigns, 1):\n",
    "        print(f\"   {i}. {campaign.name} ({campaign.status})\")\n",
    "        print(f\"      Created: {campaign.created_at}\")\n",
    "        print(f\"      ID: {campaign.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test subreddit search\n",
    "print(\"üîç Subreddit Search Example\")\n",
    "\n",
    "success, message, results = await reddit_service.discover_subreddits_by_topics(\"python programming\", limit=5)\n",
    "\n",
    "print_result(\"Subreddit Search\", success, message, {\n",
    "    \"results_count\": len(results) if results else 0\n",
    "})\n",
    "\n",
    "if success and results:\n",
    "    print(f\"\\nüéØ Found {len(results)} subreddits:\")\n",
    "    for i, subreddit in enumerate(results, 1):\n",
    "        print(f\"   {i}. r/{subreddit['name']} ({subreddit['subscribers']:,} subscribers)\")\n",
    "        print(f\"      Description: {subreddit['description'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "# Close Reddit client connection\n",
    "await reddit_service.cleanup()\n",
    "await campaign_service.cleanup()\n",
    "\n",
    "print(\"‚úÖ Resources cleaned up successfully\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìã Workflow Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"üìö Documents Ingested: {len(all_document_ids)}\")\n",
    "print(f\"   - Direct Content: {len(direct_doc_ids) if direct_doc_ids else 0}\")\n",
    "print(f\"   - File Upload: {len(file_doc_ids) if file_doc_ids else 0}\")\n",
    "print(f\"   - URL Scraping: {len(url_doc_ids)}\")\n",
    "\n",
    "if campaign_id:\n",
    "    print(f\"\\nüéØ Campaign: {campaign_id}\")\n",
    "    print(f\"üîç Topics Discovered: {len(discovered_topics)}\")\n",
    "    print(f\"üéØ Subreddits Found: {len(target_subreddits)}\")\n",
    "    print(f\"üìù Posts Analyzed: {len(target_posts)}\")\n",
    "    print(f\"üí¨ Responses Generated: {len(planned_responses)}\")\n",
    "    \n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(f\"üöÄ Responses Posted: Executed\")\n",
    "    else:\n",
    "        print(f\"üõ°Ô∏è  Responses Posted: Skipped (Safe Mode)\")\n",
    "\n",
    "print(f\"\\n‚è∞ Completed at: {datetime.now()}\")\n",
    "print(f\"‚úÖ Workflow completed successfully!\")\n",
    "\n",
    "# Display key IDs for reference\n",
    "print(\"\\nüîë Key IDs for Reference:\")\n",
    "print(f\"   Organization ID: {ORGANIZATION_ID}\")\n",
    "if campaign_id:\n",
    "    print(f\"   Campaign ID: {campaign_id}\")\n",
    "if all_document_ids:\n",
    "    print(f\"   Document IDs: {all_document_ids}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ Reddit Marketing AI Agent Workflow Complete!\")\n",
    "print(\"\\nüí° This workflow used direct function calls instead of API endpoints.\")\n",
    "print(\"   This demonstrates how to use the services programmatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete workflow of the Reddit Marketing AI Agent using **direct function calls** instead of API endpoints:\n",
    "\n",
    "1. ‚úÖ **Setup & Configuration** - Initialized all services and managers directly\n",
    "2. ‚úÖ **Organization Setup** - Used DocumentService to create/verify organization\n",
    "3. ‚úÖ **Document Ingestion** - Demonstrated all three ingestion methods via DocumentService\n",
    "4. ‚úÖ **Campaign Creation** - Created campaign using CampaignService\n",
    "5. ‚úÖ **Topic Discovery** - Extracted topics using CampaignService.discover_topics()\n",
    "6. ‚úÖ **Subreddit Discovery** - Found subreddits using CampaignService.discover_subreddits()\n",
    "7. ‚úÖ **Post Discovery** - Identified posts using CampaignService.discover_posts()\n",
    "8. ‚úÖ **Response Generation** - Generated responses using CampaignService.generate_responses()\n",
    "9. ‚úÖ **Response Execution** - Demonstrated posting workflow (with safety controls)\n",
    "10. ‚úÖ **Analytics & Reporting** - Generated reports using AnalyticsService\n",
    "\n",
    "### Key Differences from API Approach:\n",
    "- **Direct Service Access**: Called service methods directly instead of HTTP endpoints\n",
    "- **Better Performance**: No HTTP overhead, faster execution\n",
    "- **Type Safety**: Full Python type checking and IDE support\n",
    "- **Easier Debugging**: Direct access to service internals\n",
    "- **Resource Management**: Explicit cleanup of connections and resources\n",
    "\n",
    "### Service Architecture Demonstrated:\n",
    "- **DocumentService**: Document ingestion, querying, and management\n",
    "- **CampaignService**: Complete campaign workflow orchestration\n",
    "- **RedditService**: Reddit API interactions and subreddit operations\n",
    "- **LLMService**: AI-powered topic extraction and response generation\n",
    "- **AnalyticsService**: Comprehensive reporting and analytics\n",
    "- **WebScraperService**: URL content scraping capabilities\n",
    "\n",
    "### Next Steps:\n",
    "1. **Integrate into Applications**: Use these service patterns in your own applications\n",
    "2. **Customize Workflows**: Modify the services to fit your specific needs\n",
    "3. **Add Error Handling**: Implement robust error handling for production use\n",
    "4. **Scale Operations**: Use these patterns for batch processing and automation\n",
    "5. **Monitor Performance**: Add logging and monitoring to track service performance\n",
    "\n",
    "The Reddit Marketing AI Agent services are now ready for programmatic integration! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
