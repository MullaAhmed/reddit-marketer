{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Marketing AI Agent - Complete Example Workflow (Direct Functions)\n",
    "\n",
    "This notebook demonstrates the complete workflow of the Reddit Marketing AI Agent using direct function calls instead of API endpoints.\n",
    "\n",
    "## Features Demonstrated:\n",
    "1. **Setup & Configuration** - Environment validation and service initialization\n",
    "2. **Organization Setup** - Create and configure an organization\n",
    "3. **Document Ingestion** - Multiple methods (direct content, file upload, URL scraping)\n",
    "4. **Campaign Creation** - Create and configure a marketing campaign\n",
    "5. **Topic Discovery** - Extract relevant topics from documents\n",
    "6. **Subreddit Discovery** - Find relevant subreddits based on topics\n",
    "7. **Post Discovery** - Find relevant posts in target subreddits\n",
    "8. **Response Generation** - AI-generated contextual responses\n",
    "9. **Response Execution** - Post responses to Reddit (with safety controls)\n",
    "10. **Analytics & Reporting** - Comprehensive performance analysis\n",
    "\n",
    "## Safety Features:\n",
    "- **Reddit Posting Control**: `ACTUALLY_POST_TO_REDDIT = False` prevents accidental posting\n",
    "- **Credential Validation**: Checks for required API keys\n",
    "- **Error Handling**: Graceful handling of service failures\n",
    "- **Independent Cells**: Each step can be run independently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Reddit Marketing AI Agent - Example Workflow (Direct Functions)\n",
      "üìÖ Started at: 2025-06-21 07:43:58.493015\n",
      "üè¢ Organization: Example Organization (example-org-2024)\n",
      "‚ö†Ô∏è  Reddit Posting: ENABLED\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import  Any\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Add the app directory to Python path\n",
    "sys.path.append('app')\n",
    "\n",
    "# Import all necessary services and managers\n",
    "from app.services.document_service import DocumentService\n",
    "from app.services.campaign_service import CampaignService\n",
    "from app.services.reddit_service import RedditService\n",
    "from app.services.llm_service import LLMService\n",
    "from app.services.analytics_service import AnalyticsService\n",
    "from app.services.scraper_service import WebScraperService\n",
    "\n",
    "from app.managers.document_manager import DocumentManager\n",
    "from app.managers.campaign_manager import CampaignManager\n",
    "from app.managers.analytics_manager import AnalyticsManager\n",
    "\n",
    "from app.storage.json_storage import JsonStorage\n",
    "from app.storage.vector_storage import VectorStorage\n",
    "\n",
    "from app.clients.llm_client import LLMClient\n",
    "from app.clients.reddit_client import RedditClient\n",
    "from app.clients.storage_client import VectorStorageClient\n",
    "\n",
    "from app.models.campaign import (\n",
    "    CampaignCreateRequest, SubredditDiscoveryRequest, SubredditDiscoveryByTopicsRequest,\n",
    "    PostDiscoveryRequest, ResponseGenerationRequest, ResponseExecutionRequest,\n",
    "    ResponseTone\n",
    ")\n",
    "from app.models.document import DocumentCreateRequest, DocumentIngestURLRequest, DocumentQuery\n",
    "\n",
    "from app.core.settings import settings\n",
    "\n",
    "# Configuration\n",
    "ORGANIZATION_ID = \"example-org-2024\"\n",
    "ORGANIZATION_NAME = \"Example Organization\"\n",
    "\n",
    "# Safety control - Set to True only when you want to actually post to Reddit\n",
    "ACTUALLY_POST_TO_REDDIT = True\n",
    "\n",
    "# Reddit credentials (replace with your actual credentials)\n",
    "REDDIT_CREDENTIALS = {\n",
    "     \"client_id\": os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "    \"client_secret\": os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "    \"username\": os.getenv(\"REDDIT_USERNAME\"),\n",
    "    \"password\": os.getenv(\"REDDIT_PASSWORD\")\n",
    "}\n",
    "\n",
    "print(\"üöÄ Reddit Marketing AI Agent - Example Workflow (Direct Functions)\")\n",
    "print(f\"üìÖ Started at: {datetime.now()}\")\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"‚ö†Ô∏è  Reddit Posting: {'ENABLED' if ACTUALLY_POST_TO_REDDIT else 'DISABLED (Safe Mode)'}\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing Services and Managers...\n",
      "‚úÖ All services initialized successfully!\n",
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "# Initialize all services and managers\n",
    "print(\"üîß Initializing Services and Managers...\")\n",
    "\n",
    "# Storage layer\n",
    "json_storage = JsonStorage()\n",
    "vector_storage_client = VectorStorageClient()\n",
    "vector_storage = VectorStorage(vector_storage_client)\n",
    "\n",
    "# Managers\n",
    "document_manager = DocumentManager(json_storage)\n",
    "campaign_manager = CampaignManager(json_storage)\n",
    "analytics_manager = AnalyticsManager(campaign_manager, document_manager)\n",
    "\n",
    "# Clients\n",
    "llm_client = LLMClient()\n",
    "reddit_client = RedditClient(\n",
    "    client_id=REDDIT_CREDENTIALS[\"client_id\"],\n",
    "    client_secret=REDDIT_CREDENTIALS[\"client_secret\"],\n",
    "    username=REDDIT_CREDENTIALS.get(\"username\"),\n",
    "    password=REDDIT_CREDENTIALS.get(\"password\")\n",
    ")\n",
    "web_scraper_service = WebScraperService()\n",
    "\n",
    "# Services\n",
    "llm_service = LLMService(llm_client)\n",
    "reddit_service = RedditService(json_storage, reddit_client)\n",
    "document_service = DocumentService(document_manager, vector_storage, web_scraper_service)\n",
    "campaign_service = CampaignService(campaign_manager, document_service, reddit_service, llm_service)\n",
    "analytics_service = AnalyticsService(analytics_manager)\n",
    "\n",
    "print(\"‚úÖ All services initialized successfully!\")\n",
    "\n",
    "# Helper functions\n",
    "def print_result(title: str, success: bool, message: str, data: Any = None):\n",
    "    \"\"\"Pretty print service results.\"\"\"\n",
    "    print(f\"\\nüìã {title}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "    print(f\"{status} Status: {message}\")\n",
    "    \n",
    "    if data:\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                print(f\"üìä {key}: {value}\")\n",
    "        elif isinstance(data, list):\n",
    "            print(f\"üìä Items: {len(data)}\")\n",
    "            for i, item in enumerate(data[:3], 1):  # Show first 3 items\n",
    "                print(f\"   {i}. {str(item)[:80]}...\")\n",
    "        else:\n",
    "            print(f\"üìä Data: {str(data)[:100]}...\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Environment Check\n",
      "----------------------------------------\n",
      "Required API Keys:\n",
      "   ‚úÖ OPENAI_API_KEY: Set\n",
      "   ‚úÖ GOOGLE_API_KEY: Set\n",
      "\n",
      "Optional API Keys:\n",
      "   ‚úÖ GROQ_API_KEY: Set\n",
      "   ‚úÖ FIRECRAWL_API_KEY: Set\n",
      "   ‚ö†Ô∏è LANGCHAIN_PROJECT: Not set\n",
      "\n",
      "üìÅ Data Directory: data\n",
      "ü§ñ Default Model: gpt-4o\n",
      "üîç Embedding Provider: openai\n"
     ]
    }
   ],
   "source": [
    "# Check environment and settings\n",
    "print(\"üîç Environment Check\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "required_keys = {\n",
    "    \"OPENAI_API_KEY\": settings.OPENAI_API_KEY,\n",
    "    \"GOOGLE_API_KEY\": settings.GOOGLE_API_KEY\n",
    "}\n",
    "\n",
    "optional_keys = {\n",
    "    \"GROQ_API_KEY\": settings.GROQ_API_KEY,\n",
    "    \"FIRECRAWL_API_KEY\": settings.FIRECRAWL_API_KEY,\n",
    "    \"LANGCHAIN_PROJECT\": settings.LANGCHAIN_PROJECT\n",
    "}\n",
    "\n",
    "print(\"Required API Keys:\")\n",
    "for key, value in required_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Missing'}\")\n",
    "\n",
    "print(\"\\nOptional API Keys:\")\n",
    "for key, value in optional_keys.items():\n",
    "    status = \"‚úÖ\" if value else \"‚ö†Ô∏è\"\n",
    "    print(f\"   {status} {key}: {'Set' if value else 'Not set'}\")\n",
    "\n",
    "print(f\"\\nüìÅ Data Directory: {settings.DATA_DIR}\")\n",
    "print(f\"ü§ñ Default Model: {settings.MODEL_NAME}\")\n",
    "print(f\"üîç Embedding Provider: {settings.EMBEDDING_PROVIDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Organization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¢ Setting up Organization\n",
      "‚úÖ Organization: Example Organization\n",
      "üìä ID: example-org-2024\n",
      "üìÑ Documents: 0\n",
      "üìÖ Created: 2025-06-21 02:14:13.899903+00:00\n",
      "üîÑ Active: True\n",
      "\n",
      "üìã Total organizations in system: 1\n",
      "   - Example Organization (example-org-2024): 0 documents\n"
     ]
    }
   ],
   "source": [
    "# Get or create organization\n",
    "print(\"üè¢ Setting up Organization\")\n",
    "\n",
    "organization = document_service.get_or_create_organization(ORGANIZATION_ID, ORGANIZATION_NAME)\n",
    "\n",
    "print(f\"‚úÖ Organization: {organization.name}\")\n",
    "print(f\"üìä ID: {organization.id}\")\n",
    "print(f\"üìÑ Documents: {organization.documents_count}\")\n",
    "print(f\"üìÖ Created: {organization.created_at}\")\n",
    "print(f\"üîÑ Active: {organization.is_active}\")\n",
    "\n",
    "# List all organizations\n",
    "all_organizations = document_service.list_organizations()\n",
    "print(f\"\\nüìã Total organizations in system: {len(all_organizations)}\")\n",
    "for org in all_organizations:\n",
    "    print(f\"   - {org.name} ({org.id}): {org.documents_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion\n",
    "\n",
    "We'll demonstrate all three document ingestion methods:\n",
    "1. **Direct Content Input** - Paste content directly\n",
    "2. **Simulated File Upload** - Simulate uploading a file\n",
    "3. **URL Scraping** - Scrape content from a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Method 1: Direct Content Input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:01,  1.21s/it]\n",
      "Calculating embeddings: 1it [00:00,  1.08it/s]\n",
      "Document e2b78f2d-ec18-40a6-9ffd-9eaf861d1734_chunk_0 contains `meta` values of unsupported types for the keys: topics. These items will be discarded. Supported types are: str, int, float, bool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Direct Content Ingestion\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested 2 documents (2 chunks)\n",
      "üìä document_ids: ['df433591-c4ff-499b-84ea-4d9c087a9fc2', 'e2b78f2d-ec18-40a6-9ffd-9eaf861d1734']\n",
      "üìä documents_ingested: 2\n",
      "\n",
      "üìù Stored 2 document IDs from direct content\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Direct Content Input\n",
    "print(\"üìÑ Method 1: Direct Content Input\")\n",
    "\n",
    "direct_documents = [\n",
    "    {\n",
    "        \"title\": \"Python Best Practices Guide\",\n",
    "        \"content\": \"\"\"\n",
    "        Python Best Practices for Clean Code\n",
    "        \n",
    "        Writing clean, maintainable Python code is essential for any developer. Here are some key best practices:\n",
    "        \n",
    "        1. Follow PEP 8 Style Guide\n",
    "        - Use 4 spaces for indentation\n",
    "        - Keep lines under 79 characters\n",
    "        - Use descriptive variable names\n",
    "        \n",
    "        2. Write Docstrings\n",
    "        - Document all functions and classes\n",
    "        - Use triple quotes for docstrings\n",
    "        - Follow Google or NumPy docstring conventions\n",
    "        \n",
    "        3. Use Type Hints\n",
    "        - Add type hints to function parameters and return values\n",
    "        - Use typing module for complex types\n",
    "        - Helps with IDE support and code documentation\n",
    "        \n",
    "        4. Error Handling\n",
    "        - Use specific exception types\n",
    "        - Handle exceptions gracefully\n",
    "        - Log errors appropriately\n",
    "        \n",
    "        5. Testing\n",
    "        - Write unit tests for all functions\n",
    "        - Use pytest for testing framework\n",
    "        - Aim for high test coverage\n",
    "        \n",
    "        These practices will help you write more maintainable and professional Python code.\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"programming\",\n",
    "            \"language\": \"python\",\n",
    "            \"difficulty\": \"intermediate\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Machine Learning Fundamentals\",\n",
    "        \"content\": \"\"\"\n",
    "        Introduction to Machine Learning\n",
    "        \n",
    "        Machine Learning (ML) is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\n",
    "        \n",
    "        Types of Machine Learning:\n",
    "        \n",
    "        1. Supervised Learning\n",
    "        - Uses labeled training data\n",
    "        - Examples: Classification, Regression\n",
    "        - Algorithms: Linear Regression, Decision Trees, Random Forest\n",
    "        \n",
    "        2. Unsupervised Learning\n",
    "        - Works with unlabeled data\n",
    "        - Examples: Clustering, Dimensionality Reduction\n",
    "        - Algorithms: K-Means, PCA, DBSCAN\n",
    "        \n",
    "        3. Reinforcement Learning\n",
    "        - Learns through interaction with environment\n",
    "        - Uses rewards and penalties\n",
    "        - Examples: Game playing, Robotics\n",
    "        \n",
    "        Key Concepts:\n",
    "        - Feature Engineering: Selecting and transforming input variables\n",
    "        - Model Training: Teaching the algorithm using training data\n",
    "        - Model Evaluation: Testing performance on unseen data\n",
    "        - Overfitting: When model performs well on training but poorly on new data\n",
    "        \n",
    "        Popular Python Libraries:\n",
    "        - Scikit-learn: General-purpose ML library\n",
    "        - TensorFlow: Deep learning framework\n",
    "        - PyTorch: Research-focused deep learning\n",
    "        - Pandas: Data manipulation and analysis\n",
    "        - NumPy: Numerical computing\n",
    "        \"\"\",\n",
    "        \"metadata\": {\n",
    "            \"category\": \"machine-learning\",\n",
    "            \"difficulty\": \"beginner\",\n",
    "            \"topics\": [\"supervised\", \"unsupervised\", \"reinforcement\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Ingest direct content documents\n",
    "success, message, direct_doc_ids = document_service.ingest_documents(\n",
    "    documents=direct_documents,\n",
    "    org_id=ORGANIZATION_ID,\n",
    "    org_name=ORGANIZATION_NAME\n",
    ")\n",
    "\n",
    "print_result(\"Direct Content Ingestion\", success, message, {\n",
    "    \"document_ids\": direct_doc_ids,\n",
    "    \"documents_ingested\": len(direct_doc_ids) if direct_doc_ids else 0\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Stored {len(direct_doc_ids) if direct_doc_ids else 0} document IDs from direct content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Method 2: Simulated File Upload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã File Content Ingestion\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested 1 documents (1 chunks)\n",
      "üìä document_ids: ['710bc20a-ddfa-4922-a9c2-c166f7e04f53']\n",
      "üìä documents_ingested: 1\n",
      "\n",
      "üìù Stored 1 document IDs from file content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Simulated File Upload\n",
    "print(\"\\nüìÅ Method 2: Simulated File Upload\")\n",
    "\n",
    "# Simulate file content (in real scenario, this would be read from an uploaded file)\n",
    "file_content = \"\"\"\n",
    "Web Development with Python and FastAPI\n",
    "\n",
    "FastAPI is a modern, fast web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
    "\n",
    "Key Features:\n",
    "- Fast: Very high performance, on par with NodeJS and Go\n",
    "- Fast to code: Increase the speed to develop features by about 200% to 300%\n",
    "- Fewer bugs: Reduce about 40% of human (developer) induced errors\n",
    "- Intuitive: Great editor support with completion everywhere\n",
    "- Easy: Designed to be easy to use and learn\n",
    "- Short: Minimize code duplication\n",
    "- Robust: Get production-ready code with automatic interactive documentation\n",
    "\n",
    "Getting Started:\n",
    "\n",
    "1. Installation\n",
    "```bash\n",
    "pip install fastapi uvicorn\n",
    "```\n",
    "\n",
    "2. Basic Example\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"Hello\": \"World\"}\n",
    "\n",
    "@app.get(\"/items/{item_id}\")\n",
    "def read_item(item_id: int, q: str = None):\n",
    "    return {\"item_id\": item_id, \"q\": q}\n",
    "```\n",
    "\n",
    "3. Run the server\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Advanced Features:\n",
    "- Automatic API documentation with Swagger UI\n",
    "- Data validation using Pydantic models\n",
    "- Dependency injection system\n",
    "- Background tasks\n",
    "- WebSocket support\n",
    "- Authentication and authorization\n",
    "- Database integration\n",
    "\n",
    "FastAPI is perfect for building modern web APIs and microservices.\n",
    "\"\"\"\n",
    "\n",
    "# Create document from \"file\" content\n",
    "file_documents = [{\n",
    "    \"title\": \"FastAPI Web Development Guide\",\n",
    "    \"content\": file_content,\n",
    "    \"metadata\": {\n",
    "        \"source\": \"simulated_file_upload\",\n",
    "        \"filename\": \"fastapi_guide.txt\",\n",
    "        \"category\": \"web-development\",\n",
    "        \"framework\": \"fastapi\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "# Ingest file content\n",
    "success, message, file_doc_ids = document_service.ingest_documents(\n",
    "    documents=file_documents,\n",
    "    org_id=ORGANIZATION_ID\n",
    ")\n",
    "\n",
    "print_result(\"File Content Ingestion\", success, message, {\n",
    "    \"document_ids\": file_doc_ids,\n",
    "    \"documents_ingested\": len(file_doc_ids) if file_doc_ids else 0\n",
    "})\n",
    "\n",
    "print(f\"\\nüìù Stored {len(file_doc_ids) if file_doc_ids else 0} document IDs from file content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Method 3: URL Scraping\n",
      "\n",
      "üîç Scraping: https://docs.python.org/3/tutorial/introduction.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 1it [00:01,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã URL Scraping: Python Tutorial Introduction\n",
      "----------------------------------------\n",
      "‚úÖ Status: Successfully ingested document from URL: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä document_id: 8f51e877-0741-46ff-a985-70074c1f8ba3\n",
      "üìä url: https://docs.python.org/3/tutorial/introduction.html\n",
      "üìä scraping_method: auto\n",
      "\n",
      "üìù Stored 1 document IDs from URL scraping\n",
      "\n",
      "üìö Total documents ingested: 4\n",
      "Document IDs: ['df433591-c4ff-499b-84ea-4d9c087a9fc2', 'e2b78f2d-ec18-40a6-9ffd-9eaf861d1734', '710bc20a-ddfa-4922-a9c2-c166f7e04f53', '8f51e877-0741-46ff-a985-70074c1f8ba3']\n"
     ]
    }
   ],
   "source": [
    "# Method 3: URL Scraping\n",
    "print(\"\\nüåê Method 3: URL Scraping\")\n",
    "\n",
    "# Example URLs to scrape (replace with actual URLs you want to scrape)\n",
    "url_requests = [\n",
    "    {\n",
    "        \"url\": \"https://docs.python.org/3/tutorial/introduction.html\",\n",
    "        \"title\": \"Python Tutorial Introduction\",\n",
    "        \"scraping_method\": \"auto\"\n",
    "    }\n",
    "]\n",
    "\n",
    "url_doc_ids = []\n",
    "\n",
    "for url_request in url_requests:\n",
    "    print(f\"\\nüîç Scraping: {url_request['url']}\")\n",
    "    \n",
    "    # Use async function with asyncio\n",
    "    success, message, document_id = await document_service.ingest_document_from_url(\n",
    "        url=url_request[\"url\"],\n",
    "        organization_id=ORGANIZATION_ID,\n",
    "        title=url_request[\"title\"],\n",
    "        scraping_method=url_request[\"scraping_method\"]\n",
    "    )\n",
    "    \n",
    "    print_result(f\"URL Scraping: {url_request['title']}\", success, message, {\n",
    "        \"document_id\": document_id,\n",
    "        \"url\": url_request[\"url\"],\n",
    "        \"scraping_method\": url_request[\"scraping_method\"]\n",
    "    })\n",
    "    \n",
    "    if success and document_id:\n",
    "        url_doc_ids.append(document_id)\n",
    "\n",
    "print(f\"\\nüìù Stored {len(url_doc_ids)} document IDs from URL scraping\")\n",
    "\n",
    "# Combine all document IDs\n",
    "all_document_ids = (direct_doc_ids or []) + (file_doc_ids or []) + url_doc_ids\n",
    "print(f\"\\nüìö Total documents ingested: {len(all_document_ids)}\")\n",
    "print(f\"Document IDs: {all_document_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Organization Summary:\n",
      "   Name: Example Organization\n",
      "   Documents: 4\n",
      "   Created: 2025-06-21 02:14:13.899903+00:00\n",
      "\n",
      "üìà Organization Statistics:\n",
      "   Total Documents: 4\n",
      "   Total Chunks: 17\n",
      "   Total Content Length: 25,377 characters\n",
      "   Average Chunks per Document: 4.2\n"
     ]
    }
   ],
   "source": [
    "# Verify organization and documents\n",
    "updated_organization = document_service.get_or_create_organization(ORGANIZATION_ID)\n",
    "\n",
    "print(f\"üìä Organization Summary:\")\n",
    "print(f\"   Name: {updated_organization.name}\")\n",
    "print(f\"   Documents: {updated_organization.documents_count}\")\n",
    "print(f\"   Created: {updated_organization.created_at}\")\n",
    "\n",
    "# Get organization stats\n",
    "org_stats = document_service.get_organization_stats(ORGANIZATION_ID)\n",
    "if \"error\" not in org_stats:\n",
    "    print(f\"\\nüìà Organization Statistics:\")\n",
    "    print(f\"   Total Documents: {org_stats['total_documents']}\")\n",
    "    print(f\"   Total Chunks: {org_stats['total_chunks']}\")\n",
    "    print(f\"   Total Content Length: {org_stats['total_content_length']:,} characters\")\n",
    "    print(f\"   Average Chunks per Document: {org_stats['average_chunks_per_document']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Campaign Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Campaign Creation\n",
      "----------------------------------------\n",
      "‚úÖ Status: Campaign 'Python Learning Community Outreach 2024' created successfully\n",
      "\n",
      "üéØ Campaign ID: ea91c59b-d77d-40b2-8f0f-ed9e883011ef\n",
      "üìù Campaign Name: Python Learning Community Outreach 2024\n",
      "üìä Status: CampaignStatus.CREATED\n",
      "üéµ Tone: ResponseTone.HELPFUL\n",
      "üìÖ Created: 2025-06-21 02:14:57.933579+00:00\n"
     ]
    }
   ],
   "source": [
    "# Create a new campaign\n",
    "campaign_request = CampaignCreateRequest(\n",
    "    name=\"Python Learning Community Outreach 2024\",\n",
    "    description=\"Engage with Python learning communities to share knowledge and best practices\",\n",
    "    response_tone=ResponseTone.HELPFUL,\n",
    "    max_responses_per_day=5\n",
    ")\n",
    "\n",
    "success, message, campaign = await campaign_service.create_campaign(\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    request=campaign_request\n",
    ")\n",
    "\n",
    "print_result(\"Campaign Creation\", success, message)\n",
    "\n",
    "# Store campaign for later use\n",
    "campaign_id = None\n",
    "if success and campaign:\n",
    "    campaign_id = campaign.id\n",
    "    print(f\"\\nüéØ Campaign ID: {campaign_id}\")\n",
    "    print(f\"üìù Campaign Name: {campaign.name}\")\n",
    "    print(f\"üìä Status: {campaign.status}\")\n",
    "    print(f\"üéµ Tone: {campaign.response_tone}\")\n",
    "    print(f\"üìÖ Created: {campaign.created_at}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to create campaign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Topic Discovery\n",
    "\n",
    "First, we'll extract relevant topics from our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Step 1: Discovering Topics from Documents\n",
      "\n",
      "üìã Topic Discovery\n",
      "----------------------------------------\n",
      "‚úÖ Status: Extracted 10 topics from 3 documents\n",
      "üìä topics: ['Python best practices', 'PEP 8 style guide', 'Machine learning', 'Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Web development with Python', 'FastAPI', 'Python testing', 'Error handling in Python']\n",
      "üìä selected_document_ids: ['df433591-c4ff-499b-84ea-4d9c087a9fc2', 'e2b78f2d-ec18-40a6-9ffd-9eaf861d1734', '710bc20a-ddfa-4922-a9c2-c166f7e04f53']\n",
      "üìä total_topics: 10\n",
      "\n",
      "üìã Discovered Topics:\n",
      "   1. Python best practices\n",
      "   2. PEP 8 style guide\n",
      "   3. Machine learning\n",
      "   4. Supervised learning\n",
      "   5. Unsupervised learning\n",
      "   6. Reinforcement learning\n",
      "   7. Web development with Python\n",
      "   8. FastAPI\n",
      "   9. Python testing\n",
      "   10. Error handling in Python\n",
      "\n",
      "üìä Campaign Status: CampaignStatus.DOCUMENTS_UPLOADED\n",
      "üìÑ Documents Selected: 3\n"
     ]
    }
   ],
   "source": [
    "if campaign_id and all_document_ids:\n",
    "    print(\"üîç Step 1: Discovering Topics from Documents\")\n",
    "    \n",
    "    # Discover topics from selected documents\n",
    "    topic_discovery_request = SubredditDiscoveryRequest(\n",
    "        document_ids=all_document_ids[:3]  # Use first 3 documents for topic discovery\n",
    "    )\n",
    "    \n",
    "    success, message, topic_data = await campaign_service.discover_topics(\n",
    "        campaign_id=campaign_id,\n",
    "        request=topic_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Topic Discovery\", success, message, topic_data)\n",
    "    \n",
    "    # Extract topics for next step\n",
    "    discovered_topics = []\n",
    "    if success and topic_data and \"topics\" in topic_data:\n",
    "        discovered_topics = topic_data[\"topics\"]\n",
    "        print(f\"\\nüìã Discovered Topics:\")\n",
    "        for i, topic in enumerate(discovered_topics, 1):\n",
    "            print(f\"   {i}. {topic}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üìÑ Documents Selected: {len(updated_campaign.selected_document_ids)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or document IDs\")\n",
    "    discovered_topics = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Subreddit Discovery\n",
    "\n",
    "Now we'll use the discovered topics to find relevant subreddits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Step 2: Discovering Subreddits from Topics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error getting details for r/thisisthewayitwillbe: received 403 HTTP response\n",
      "Error: received 403 HTTP response. Retrying in 2.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 4.00 seconds...\n",
      "Error: received 403 HTTP response. Retrying in 8.00 seconds...\n",
      "Max retries reached: received 403 HTTP response\n",
      "Error getting info for r/positivereinforcement: received 403 HTTP response\n",
      "Error getting details for r/positivereinforcement: received 403 HTTP response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Subreddit Discovery\n",
      "----------------------------------------\n",
      "‚úÖ Status: Discovered 10 relevant subreddits\n",
      "üìä subreddits: ['Python', 'learnpython', 'pythontips', 'MachineLearning', 'learnmachinelearning', 'datascience', 'artificial', 'ArtificialInteligence', 'deeplearning', 'FastAPI']\n",
      "üìä topics: ['Python best practices', 'PEP 8 style guide', 'Machine learning', 'Supervised learning', 'Unsupervised learning', 'Reinforcement learning', 'Web development with Python', 'FastAPI', 'Python testing', 'Error handling in Python']\n",
      "üìä total_found: 10\n",
      "\n",
      "üéØ Target Subreddits:\n",
      "   1. r/Python\n",
      "   2. r/learnpython\n",
      "   3. r/pythontips\n",
      "   4. r/MachineLearning\n",
      "   5. r/learnmachinelearning\n",
      "   6. r/datascience\n",
      "   7. r/artificial\n",
      "   8. r/ArtificialInteligence\n",
      "   9. r/deeplearning\n",
      "   10. r/FastAPI\n",
      "\n",
      "üìä Campaign Status: CampaignStatus.SUBREDDITS_DISCOVERED\n",
      "üéØ Subreddits Found: 10\n"
     ]
    }
   ],
   "source": [
    "if campaign_id and discovered_topics:\n",
    "    print(\"üéØ Step 2: Discovering Subreddits from Topics\")\n",
    "    \n",
    "    # Discover subreddits based on topics\n",
    "    subreddit_discovery_request = SubredditDiscoveryByTopicsRequest(\n",
    "        topics=discovered_topics\n",
    "    )\n",
    "    \n",
    "    success, message, subreddit_data = await campaign_service.discover_subreddits(\n",
    "        campaign_id=campaign_id,\n",
    "        request=subreddit_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Subreddit Discovery\", success, message, subreddit_data)\n",
    "    \n",
    "    # Extract subreddits for next step\n",
    "    target_subreddits = []\n",
    "    if success and subreddit_data and \"subreddits\" in subreddit_data:\n",
    "        target_subreddits = subreddit_data[\"subreddits\"]\n",
    "        print(f\"\\nüéØ Target Subreddits:\")\n",
    "        for i, subreddit in enumerate(target_subreddits, 1):\n",
    "            print(f\"   {i}. r/{subreddit}\")\n",
    "    \n",
    "    # Check updated campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üéØ Subreddits Found: {len(updated_campaign.target_subreddits)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or topics\")\n",
    "    target_subreddits = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_subreddits:\n",
    "    print(\"üìù Step 3: Discovering Relevant Posts\")\n",
    "    \n",
    "    # Discover posts in target subreddits\n",
    "    post_discovery_request = PostDiscoveryRequest(\n",
    "        subreddits=target_subreddits[:3],  # Limit to first 3 subreddits for demo\n",
    "        max_posts_per_subreddit=5,\n",
    "        time_filter=\"week\",\n",
    "        reddit_credentials=REDDIT_CREDENTIALS\n",
    "    )\n",
    "    \n",
    "    success, message, posts_data = await campaign_service.discover_posts(\n",
    "        campaign_id=campaign_id,\n",
    "        request=post_discovery_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Post Discovery\", success, message, posts_data)\n",
    "    \n",
    "    # Extract post information\n",
    "    target_posts = []\n",
    "    if success and posts_data and \"posts\" in posts_data:\n",
    "        target_posts = posts_data[\"posts\"]\n",
    "        print(f\"\\nüìù Found {len(target_posts)} relevant posts:\")\n",
    "        for i, post in enumerate(target_posts[:5], 1):  # Show first 5\n",
    "            print(f\"   {i}. r/{post['subreddit']}: {post['title'][:60]}...\")\n",
    "            print(f\"      Relevance: {post['relevance_score']:.2f} - {post['relevance_reason']}\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üìù Posts Found: {len(updated_campaign.target_posts)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or subreddits\")\n",
    "    target_posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and target_posts:\n",
    "    print(\"üí¨ Step 4: Generating Responses\")\n",
    "    \n",
    "    # Get post IDs for response generation\n",
    "    post_ids = [post[\"id\"] for post in target_posts[:3]]  # Limit to first 3 posts\n",
    "    \n",
    "    response_generation_request = ResponseGenerationRequest(\n",
    "        target_post_ids=post_ids,\n",
    "        tone=ResponseTone.HELPFUL\n",
    "    )\n",
    "    \n",
    "    success, message, generation_data = await campaign_service.generate_responses(\n",
    "        campaign_id=campaign_id,\n",
    "        request=response_generation_request\n",
    "    )\n",
    "    \n",
    "    print_result(\"Response Generation\", success, message, generation_data)\n",
    "    \n",
    "    # Show generated responses\n",
    "    planned_responses = []\n",
    "    if success and generation_data and \"responses\" in generation_data:\n",
    "        planned_responses = generation_data[\"responses\"]\n",
    "        print(f\"\\nüí¨ Generated {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"\\n   Response {i}:\")\n",
    "            print(f\"   Target Post: {response['target_post_id']}\")\n",
    "            print(f\"   Confidence: {response['confidence_score']:.2f}\")\n",
    "            print(f\"   Content Preview: {response['response_content'][:100]}...\")\n",
    "    \n",
    "    # Check campaign status\n",
    "    success_status, message_status, updated_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "    if success_status and updated_campaign:\n",
    "        print(f\"\\nüìä Campaign Status: {updated_campaign.status}\")\n",
    "        print(f\"üí¨ Responses Planned: {len(updated_campaign.planned_responses)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or posts\")\n",
    "    planned_responses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Response Execution (Optional)\n",
    "\n",
    "‚ö†Ô∏è **WARNING**: This step will actually post to Reddit if `ACTUALLY_POST_TO_REDDIT = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if campaign_id and planned_responses:\n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(\"üöÄ Step 5: Executing Responses (POSTING TO REDDIT)\")\n",
    "        \n",
    "        # Get response IDs for execution\n",
    "        response_ids = [response[\"id\"] for response in planned_responses[:2]]  # Limit to first 2\n",
    "        \n",
    "        execution_request = ResponseExecutionRequest(\n",
    "            planned_response_ids=response_ids,\n",
    "            reddit_credentials=REDDIT_CREDENTIALS\n",
    "        )\n",
    "        \n",
    "        success, message, execution_data = await campaign_service.execute_responses(\n",
    "            campaign_id=campaign_id,\n",
    "            request=execution_request\n",
    "        )\n",
    "        \n",
    "        print_result(\"Response Execution\", success, message, execution_data)\n",
    "        \n",
    "        # Show execution results\n",
    "        if success and execution_data and \"posted_responses\" in execution_data:\n",
    "            posted_responses = execution_data[\"posted_responses\"]\n",
    "            print(f\"\\nüöÄ Execution Results:\")\n",
    "            for i, response in enumerate(posted_responses, 1):\n",
    "                status = \"‚úÖ Success\" if response[\"posting_successful\"] else \"‚ùå Failed\"\n",
    "                print(f\"   Response {i}: {status}\")\n",
    "                if response[\"posting_successful\"]:\n",
    "                    print(f\"   Reddit URL: https://reddit.com{response['reddit_permalink']}\")\n",
    "                else:\n",
    "                    print(f\"   Error: {response.get('error_message', 'Unknown error')}\")\n",
    "        \n",
    "        # Final campaign status\n",
    "        success_status, message_status, final_campaign = await campaign_service.get_campaign(campaign_id)\n",
    "        if success_status and final_campaign:\n",
    "            print(f\"\\nüìä Final Campaign Status: {final_campaign.status}\")\n",
    "            print(f\"üöÄ Responses Posted: {len(final_campaign.posted_responses)}\")\n",
    "            successful_posts = len([r for r in final_campaign.posted_responses.values() if r.posting_successful])\n",
    "            failed_posts = len([r for r in final_campaign.posted_responses.values() if not r.posting_successful])\n",
    "            print(f\"‚úÖ Successful Posts: {successful_posts}\")\n",
    "            print(f\"‚ùå Failed Posts: {failed_posts}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Step 5: Response Execution SKIPPED (Safe Mode)\")\n",
    "        print(\"\\nüõ°Ô∏è  Reddit posting is disabled for safety.\")\n",
    "        print(\"   To enable posting, set ACTUALLY_POST_TO_REDDIT = True\")\n",
    "        print(\"   and provide valid Reddit credentials.\")\n",
    "        \n",
    "        print(f\"\\nüìã Would have posted {len(planned_responses)} responses:\")\n",
    "        for i, response in enumerate(planned_responses, 1):\n",
    "            print(f\"   {i}. Response with confidence {response['confidence_score']:.2f}\")\n",
    "            print(f\"      Content: {response['response_content'][:80]}...\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot proceed: Missing campaign ID or planned responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analytics & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Step 6: Analytics & Reporting\")\n",
    "\n",
    "# Get organization quick stats\n",
    "quick_stats = analytics_service.get_quick_stats(ORGANIZATION_ID)\n",
    "print_result(\"Organization Quick Stats\", \"error\" not in quick_stats, \"Quick stats retrieved\", quick_stats)\n",
    "\n",
    "# Get organization performance report\n",
    "performance_report = analytics_service.get_organization_performance_report(ORGANIZATION_ID)\n",
    "print_result(\"Organization Performance Report\", \"error\" not in performance_report, \"Performance report generated\", {\n",
    "    \"report_type\": performance_report.get(\"report_type\"),\n",
    "    \"insights_count\": len(performance_report.get(\"performance_insights\", [])),\n",
    "    \"campaign_stats\": performance_report.get(\"campaign_stats\", {})\n",
    "})\n",
    "\n",
    "if campaign_id:\n",
    "    # Get campaign engagement report\n",
    "    engagement_report = analytics_service.get_campaign_engagement_report(campaign_id)\n",
    "    print_result(\"Campaign Engagement Report\", \"error\" not in engagement_report, \"Engagement report generated\", {\n",
    "        \"campaign_name\": engagement_report.get(\"campaign_name\"),\n",
    "        \"status\": engagement_report.get(\"status\"),\n",
    "        \"basic_stats\": engagement_report.get(\"basic_stats\", {})\n",
    "    })\n",
    "\n",
    "# Get platform overview\n",
    "platform_overview = analytics_service.get_overall_platform_metrics()\n",
    "print_result(\"Platform Overview\", \"error\" not in platform_overview, \"Platform overview generated\", {\n",
    "    \"total_campaigns\": platform_overview.get(\"campaign_stats\", {}).get(\"total_campaigns\", 0),\n",
    "    \"total_organizations\": platform_overview.get(\"campaign_stats\", {}).get(\"total_organizations\", 0),\n",
    "    \"insights_count\": len(platform_overview.get(\"platform_insights\", []))\n",
    "})\n",
    "\n",
    "# Get subreddit effectiveness report\n",
    "subreddit_effectiveness = analytics_service.get_subreddit_effectiveness_report(ORGANIZATION_ID)\n",
    "print_result(\"Subreddit Effectiveness\", \"error\" not in subreddit_effectiveness, \"Subreddit effectiveness analyzed\", {\n",
    "    \"total_subreddits\": subreddit_effectiveness.get(\"total_subreddits_analyzed\", 0),\n",
    "    \"recommendations_count\": len(subreddit_effectiveness.get(\"recommendations\", []))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Additional Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query documents\n",
    "print(\"üîç Document Query Example\")\n",
    "\n",
    "query = DocumentQuery(\n",
    "    query=\"machine learning algorithms\",\n",
    "    organization_id=ORGANIZATION_ID,\n",
    "    method=\"semantic\",\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "query_response = document_service.query_documents(query)\n",
    "\n",
    "print_result(\"Document Query\", True, f\"Found {query_response.total_results} documents\", {\n",
    "    \"query\": query_response.query,\n",
    "    \"method\": query_response.method,\n",
    "    \"processing_time_ms\": query_response.processing_time_ms,\n",
    "    \"total_results\": query_response.total_results\n",
    "})\n",
    "\n",
    "if query_response.documents:\n",
    "    print(f\"\\nüìÑ Found {len(query_response.documents)} relevant documents:\")\n",
    "    for i, doc in enumerate(query_response.documents, 1):\n",
    "        print(f\"   {i}. {doc.title} (Score: {doc.score:.3f})\")\n",
    "        print(f\"      Content: {doc.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all campaigns for the organization\n",
    "print(\"üìã List All Campaigns\")\n",
    "\n",
    "success, message, campaigns = await campaign_service.list_campaigns(ORGANIZATION_ID)\n",
    "\n",
    "print_result(\"Organization Campaigns\", success, message, {\n",
    "    \"campaigns_count\": len(campaigns) if campaigns else 0\n",
    "})\n",
    "\n",
    "if success and campaigns:\n",
    "    print(f\"\\nüìä Found {len(campaigns)} campaigns:\")\n",
    "    for i, campaign in enumerate(campaigns, 1):\n",
    "        print(f\"   {i}. {campaign.name} ({campaign.status})\")\n",
    "        print(f\"      Created: {campaign.created_at}\")\n",
    "        print(f\"      ID: {campaign.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test subreddit search\n",
    "print(\"üîç Subreddit Search Example\")\n",
    "\n",
    "success, message, results = await reddit_service.search_subreddits(\"python programming\", limit=5)\n",
    "\n",
    "print_result(\"Subreddit Search\", success, message, {\n",
    "    \"results_count\": len(results) if results else 0\n",
    "})\n",
    "\n",
    "if success and results:\n",
    "    print(f\"\\nüéØ Found {len(results)} subreddits:\")\n",
    "    for i, subreddit in enumerate(results, 1):\n",
    "        print(f\"   {i}. r/{subreddit['name']} ({subreddit['subscribers']:,} subscribers)\")\n",
    "        print(f\"      Description: {subreddit['description'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup resources\n",
    "print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "# Close Reddit client connection\n",
    "await reddit_service.cleanup()\n",
    "await campaign_service.cleanup()\n",
    "\n",
    "print(\"‚úÖ Resources cleaned up successfully\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nüìã Workflow Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üè¢ Organization: {ORGANIZATION_NAME} ({ORGANIZATION_ID})\")\n",
    "print(f\"üìö Documents Ingested: {len(all_document_ids)}\")\n",
    "print(f\"   - Direct Content: {len(direct_doc_ids) if direct_doc_ids else 0}\")\n",
    "print(f\"   - File Upload: {len(file_doc_ids) if file_doc_ids else 0}\")\n",
    "print(f\"   - URL Scraping: {len(url_doc_ids)}\")\n",
    "\n",
    "if campaign_id:\n",
    "    print(f\"\\nüéØ Campaign: {campaign_id}\")\n",
    "    print(f\"üîç Topics Discovered: {len(discovered_topics)}\")\n",
    "    print(f\"üéØ Subreddits Found: {len(target_subreddits)}\")\n",
    "    print(f\"üìù Posts Analyzed: {len(target_posts)}\")\n",
    "    print(f\"üí¨ Responses Generated: {len(planned_responses)}\")\n",
    "    \n",
    "    if ACTUALLY_POST_TO_REDDIT:\n",
    "        print(f\"üöÄ Responses Posted: Executed\")\n",
    "    else:\n",
    "        print(f\"üõ°Ô∏è  Responses Posted: Skipped (Safe Mode)\")\n",
    "\n",
    "print(f\"\\n‚è∞ Completed at: {datetime.now()}\")\n",
    "print(f\"‚úÖ Workflow completed successfully!\")\n",
    "\n",
    "# Display key IDs for reference\n",
    "print(\"\\nüîë Key IDs for Reference:\")\n",
    "print(f\"   Organization ID: {ORGANIZATION_ID}\")\n",
    "if campaign_id:\n",
    "    print(f\"   Campaign ID: {campaign_id}\")\n",
    "if all_document_ids:\n",
    "    print(f\"   Document IDs: {all_document_ids}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"üéâ Reddit Marketing AI Agent Workflow Complete!\")\n",
    "print(\"\\nüí° This workflow used direct function calls instead of API endpoints.\")\n",
    "print(\"   This demonstrates how to use the services programmatically.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete workflow of the Reddit Marketing AI Agent using **direct function calls** instead of API endpoints:\n",
    "\n",
    "1. ‚úÖ **Setup & Configuration** - Initialized all services and managers directly\n",
    "2. ‚úÖ **Organization Setup** - Used DocumentService to create/verify organization\n",
    "3. ‚úÖ **Document Ingestion** - Demonstrated all three ingestion methods via DocumentService\n",
    "4. ‚úÖ **Campaign Creation** - Created campaign using CampaignService\n",
    "5. ‚úÖ **Topic Discovery** - Extracted topics using CampaignService.discover_topics()\n",
    "6. ‚úÖ **Subreddit Discovery** - Found subreddits using CampaignService.discover_subreddits()\n",
    "7. ‚úÖ **Post Discovery** - Identified posts using CampaignService.discover_posts()\n",
    "8. ‚úÖ **Response Generation** - Generated responses using CampaignService.generate_responses()\n",
    "9. ‚úÖ **Response Execution** - Demonstrated posting workflow (with safety controls)\n",
    "10. ‚úÖ **Analytics & Reporting** - Generated reports using AnalyticsService\n",
    "\n",
    "### Key Differences from API Approach:\n",
    "- **Direct Service Access**: Called service methods directly instead of HTTP endpoints\n",
    "- **Better Performance**: No HTTP overhead, faster execution\n",
    "- **Type Safety**: Full Python type checking and IDE support\n",
    "- **Easier Debugging**: Direct access to service internals\n",
    "- **Resource Management**: Explicit cleanup of connections and resources\n",
    "\n",
    "### Service Architecture Demonstrated:\n",
    "- **DocumentService**: Document ingestion, querying, and management\n",
    "- **CampaignService**: Complete campaign workflow orchestration\n",
    "- **RedditService**: Reddit API interactions and subreddit operations\n",
    "- **LLMService**: AI-powered topic extraction and response generation\n",
    "- **AnalyticsService**: Comprehensive reporting and analytics\n",
    "- **WebScraperService**: URL content scraping capabilities\n",
    "\n",
    "### Next Steps:\n",
    "1. **Integrate into Applications**: Use these service patterns in your own applications\n",
    "2. **Customize Workflows**: Modify the services to fit your specific needs\n",
    "3. **Add Error Handling**: Implement robust error handling for production use\n",
    "4. **Scale Operations**: Use these patterns for batch processing and automation\n",
    "5. **Monitor Performance**: Add logging and monitoring to track service performance\n",
    "\n",
    "The Reddit Marketing AI Agent services are now ready for programmatic integration! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
